<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Zhanliang Liu on Zhanliang Liu</title>
    <link>/</link>
    <description>Recent content in Zhanliang Liu on Zhanliang Liu</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 +0800</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>⛵ Learning Meaning in Natural Language Processing — The Semantics Mega-Thread</title>
      <link>/post/semantic-meaning/</link>
      <pubDate>Thu, 15 Nov 2018 00:00:00 +0800</pubDate>
      
      <guid>/post/semantic-meaning/</guid>
      <description>

&lt;p&gt;Last week &lt;a href=&#34;https://twitter.com/jacobandreas/status/1023246560082063366&#34; target=&#34;_blank&#34;&gt;a tweet by Jacob Andreas&lt;/a&gt; triggered a huge discussion on Twitter that many people have called the &lt;em&gt;meaning/semantics mega-thread&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id=&#34;a-crash-course-on-lexical-meaning-semantics&#34;&gt;🏎 A crash course on Lexical Meaning &amp;amp; Semantics&lt;/h2&gt;

&lt;p&gt;If you already know what we mean by &lt;em&gt;“Meaning/Semantics”&lt;/em&gt; in NLP, you can skip this part and go straight to the debate 🔥.&lt;/p&gt;

&lt;p&gt;For the CS/ML folks out there here are a few words of introduction.&lt;/p&gt;

&lt;p&gt;First, it’s important to state that Meaning in Natural Language is a multi-facetted concept with semantic, pragmatic, cognitive and social aspects. The discussion that happened on Twitter was mainly about lexical semantics and compositionality so I will focus on this sub-field for brevity. You will find additional links to broaden this view at the end of this section.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Meaning_%28linguistics%29&#34; target=&#34;_blank&#34;&gt;Meaning&lt;/a&gt; is the information that a sender intends to convey, or does convey, to a receiver.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Now, we know that &lt;strong&gt;strings are already a representation of meaning&lt;/strong&gt;, so why should we go any further than just raw text?&lt;/p&gt;

&lt;p&gt;Well there are several reasons we may want to distinguish meaning from raw text.&lt;/p&gt;

&lt;p&gt;One reason is that the field of NLP/NLU aims at &lt;strong&gt;building systems&lt;/strong&gt; that understand what you say to them, trigger actions based on that and convey back meaningful information. Let’s take a simple example:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Context: Knowledge of mathematics&lt;/li&gt;
&lt;li&gt;Utterance: &lt;em&gt;What is the largest prime less than 10?&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Action: &lt;code&gt;7&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Given some knowledge of math, we want our NLU system to produce an appropriate answer.&lt;/p&gt;

&lt;p&gt;It’s difficult to (i) link raw text to a knowledge base of mathematical facts in our system and (ii) combine pieces of knowledge together to infer an answer. One solution is to define an intermediate meaning representation (sometimes called a Logical Form) that is more easy to manipulate.&lt;/p&gt;

&lt;p&gt;For example in our case:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Meaning representation: 𝑚𝑎𝑥(𝚙𝚛𝚒𝚖𝚎𝚜 ∩(−∞; 𝟣𝟢))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can then execute this expression with respect to a model of the world, like our database of knowledge, to get an answer. This way, we have also factored out the understanding of language (called &lt;em&gt;semantic parsing&lt;/em&gt;) from the world knowledge (the problem of grounding meaning of in the real word).&lt;/p&gt;

&lt;p&gt;Advantageously, our &lt;em&gt;representation of the meaning of a sentence&lt;/em&gt; can thus:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;provide a way to &lt;strong&gt;link language&lt;/strong&gt; to external &lt;strong&gt;knowledge base&lt;/strong&gt;, &lt;strong&gt;observations&lt;/strong&gt;, and &lt;strong&gt;actions&lt;/strong&gt;;&lt;/li&gt;
&lt;li&gt;support &lt;strong&gt;computational inference&lt;/strong&gt;, so that concepts can be &lt;strong&gt;combined&lt;/strong&gt; to derive additional knowledge as human do during a conversation.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Two other nice requirements for this representation:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;unambiguous&lt;/strong&gt;: one meaning per statement (unlike natural language);&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;expressive&lt;/strong&gt; enough to cover the full range of things that people talk about.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Natural language as raw text doesn’t fulfill most of these criteria!&lt;/p&gt;

&lt;p&gt;A related line of research is &lt;strong&gt;Formal Semantics&lt;/strong&gt; which seek to understand linguistic meaning by constructing models of the principles that speakers use to convey meaning.&lt;/p&gt;

&lt;p&gt;The tools of formal semantics are similar to NLU/NLP tools but the aim is to understand how people construct meaning more than any specific application.&lt;/p&gt;

&lt;p&gt;Now there is a lot more to meaning than just logic forms and grounding. A few examples: &lt;strong&gt;“But I didn’t mean it literally!!”&lt;/strong&gt; (speaker meaning ≠ literal meaning), &lt;strong&gt;“Buffalo buffalo Buffalo buffalo buffalo buffalo Buffalo buffalo.”&lt;/strong&gt; (yes, this is a real sentence with a meaning but you need to find the right sense for each buffalo!) and so on&amp;hellip;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;A few pointers: Our simple example came from &lt;a href=&#34;https://cs.stanford.edu/~pliang/papers/executable-cacm2016.pdf&#34; target=&#34;_blank&#34;&gt;this nice article by Percy Liang&lt;/a&gt;. As a quick overview of the field, I would recommend chapters 12 and 13 of J. Eisenstein’s book &lt;a href=&#34;https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf&#34; target=&#34;_blank&#34;&gt;“Natural Language Processing”&lt;/a&gt;. They will take you through the main ideas, tools up to recent research on Meaning in NLP. &lt;a href=&#34;http://faculty.washington.edu/ebender/100things-sem_prag.html&#34; target=&#34;_blank&#34;&gt;Emily M. Bender’s ACL 2018 tutorial&lt;/a&gt; is a nice way to see how Meaning can be a multi-headed monster 🐍 to say the least!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Now back to our mega-thread!&lt;/p&gt;

&lt;h2 id=&#34;triggering-a-debate-on-meaning&#34;&gt;🔥 Triggering a debate on Meaning&lt;/h2&gt;

&lt;p&gt;As often, the discussion was sparked by a mention of sentence embeddings.&lt;/p&gt;

&lt;p&gt;This argument was the main trigger for the mega-discussion that followed. Further in the thread, Emily M. Bender reformulates her argument as:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;If all the learner gets is text the learner cannot learn meaning.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;can-a-model-trained-only-on-raw-text-learn-meaning&#34;&gt;Can a model trained only on raw text learn meaning?&lt;/h2&gt;

&lt;p&gt;This question was explored along two axes:&lt;/p&gt;

&lt;h3 id=&#34;what-aspect-of-meaning-can-a-model-learn&#34;&gt;What aspect of Meaning can a model learn?&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Can it learn &lt;em&gt;meaning&lt;/em&gt; or just learn &lt;em&gt;similarity&lt;/em&gt; in meaning (i.e. learn that some expressions are similar without knowing what they mean. It is still very useful for transfer learning)?&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Can it learn &lt;em&gt;grounded meaning&lt;/em&gt; (learn the meaning of each expression as a state of the world state) or learn &lt;em&gt;lexical meaning&lt;/em&gt; (e.g. learn how the meaning of sub-expressions compose together, as in our logical forms)?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;how-can-the-model-learn&#34;&gt;How can the model Learn?&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;If the model cannot learn meaning from raw text alone, what would be the &lt;em&gt;minimal amount of additional supervision&lt;/em&gt; needed? Should we add supervision from Logical Forms, Textual Entailment&amp;hellip;?&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Could we encode some &lt;em&gt;inductive bias&lt;/em&gt; in the model so that it can learn aspects of meaning from raw text?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;the-thai-and-java-experiments&#34;&gt;The Thai and Java Experiments&lt;/h2&gt;

&lt;p&gt;Emily M. Bender proposed several interesting experiments which were discussed at length:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://twitter.com/emilymbender/status/1024042044035985408&#34; target=&#34;_blank&#34;&gt;The Thai Room experiment&lt;/a&gt;: &lt;em&gt;“Imagine [you] were given the sum total of all Thai literature in a huge library. (All in Thai, no translations.) Assuming you don’t already know Thai, you won’t learn it from that.”&lt;/em&gt; &lt;strong&gt;A real life example of trying to learn from raw text only.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://twitter.com/emilymbender/status/1025002835467890689&#34; target=&#34;_blank&#34;&gt;The Java Code experiment&lt;/a&gt;: &lt;em&gt;“Give your NN all well-formed java code that’s ever been written, but only the surface form of the code. Then ask it to evaluate (i.e. execute) part of it.”&lt;/em&gt; &lt;strong&gt;Can we learn execution semantics from raw text only?&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;investigating-programming-language-semantic&#34;&gt;Investigating Programming Language Semantic&lt;/h2&gt;

&lt;p&gt;The Java Code proposal triggered an interesting discussion on the difference between trying to learn meaning from Programming Language (PL) code and from Natural Language (NL) text.&lt;/p&gt;

&lt;p&gt;It actually seems more difficult to learn from PL than NL.&lt;/p&gt;

&lt;p&gt;Learning meaning from Java code is like having a text composed only of orders/commands and without any descriptions. But description are very important feedbacks for learning as they allow one to compare its internal world state with the real world state.&lt;/p&gt;

&lt;h2 id=&#34;language-models&#34;&gt;Language Models&lt;/h2&gt;

&lt;p&gt;The discussion circled around Language Models. A language model is a model which can &lt;strong&gt;predict the next word in a sentence given a context&lt;/strong&gt; (past words, external knowledge). Recently, these models have given &lt;a href=&#34;https://blog.openai.com/language-unsupervised/&#34; target=&#34;_blank&#34;&gt;interesting results&lt;/a&gt; in commonsense reasoning. Language models were examined in two settings:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Independently of any training dataset&lt;/strong&gt;: having a &lt;em&gt;human-level language model&lt;/em&gt; involves that the model has a human-like notion of meaning and world state. As &lt;a href=&#34;https://twitter.com/yoavgo/status/1025485692347056128&#34; target=&#34;_blank&#34;&gt;Yolav Goldberg mentioned&lt;/a&gt; &lt;em&gt;“it’s just (one of the many) trivial examples where perfect LM entails solving not only all of language but also all of AI.”&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;More interesting is the case of a language model &lt;strong&gt;trained from raw text only&lt;/strong&gt;: here the question is how much the model can learn in terms of semantics without being given access to explicit meaning information!&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;textual-entailment&#34;&gt;Textual Entailment&lt;/h2&gt;

&lt;p&gt;One way to give some information about meaning without departing too much from raw text is to train a model on Textual Entailment, the task of &lt;strong&gt;predicting whether a sentence imply another&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;In a series tweets, Sam Bowman explained his view that entailment could be used to learn “the meat of compositional” semantics almost from raw text.&lt;/p&gt;

&lt;p&gt;And also made the suggestion that a learner might be able to learn entailment in a simpler way than curent setups, using a setup that could be close to LM.&lt;/p&gt;

&lt;h2 id=&#34;an-inductive-bias-language-model&#34;&gt;An Inductive Bias Language Model&lt;/h2&gt;

&lt;p&gt;Another way to learn meaning from a dataset as close as possible to raw text is to put a strong inductive bias in the model as discussed by Matt Gardner.&lt;/p&gt;

&lt;p&gt;One example is the &lt;a href=&#34;https://arxiv.org/abs/1708.00781&#34; target=&#34;_blank&#34;&gt;Entity Language Model&lt;/a&gt; which augments a classical LSTM by explicitly modeling an arbitrary number of entities, updating their representations along the hidden state, and using the mention representations to contextually generate the next word in the LM task.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;To read more about that, check &lt;a href=&#34;https://sites.google.com/site/repl4nlp2018/&#34; target=&#34;_blank&#34;&gt;Yejin Choi’s talk at ACL 2018&lt;/a&gt; and &lt;a href=&#34;https://www.youtube.com/watch?v=7CcSm0PAr-Y&#34; target=&#34;_blank&#34;&gt;Percy Liang’s talk at AAAI 2018&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;the-big-open-question&#34;&gt;The Big Open Question&lt;/h2&gt;

&lt;p&gt;In the end, I feel like the main original question stayed open: can a model learn some aspects of lexical meaning from raw text alone?&lt;/p&gt;

&lt;h2 id=&#34;a-word-on-searle-s-chinese-room&#34;&gt;A word on Searle’s Chinese Room&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Chinese_room&#34; target=&#34;_blank&#34;&gt;Searle’s room argument&lt;/a&gt; came back often in the discussion but the situation was a bit different.&lt;/p&gt;

&lt;p&gt;Searle’s argument was made in the Strong versus Weak AI debate: does the computer has a mind or consciousness. Here the question is less philosophical: can we extract a representation of meaning from form alone.&lt;/p&gt;

&lt;p&gt;Still, &lt;a href=&#34;https://twitter.com/jeremyphoward/status/1026881686359822337&#34; target=&#34;_blank&#34;&gt;as Jeremy Howard detailed a bit later&lt;/a&gt;, the Chinese room experiment of Searle goes far beyond the question of Strong/weak AI to the question of understanding/qualia, so please go &lt;a href=&#34;https://twitter.com/jeremyphoward/status/1026881686359822337&#34; target=&#34;_blank&#34;&gt;check this thread&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Filtered-Space Saving Top-K</title>
      <link>/post/topk-in-stream/</link>
      <pubDate>Mon, 25 Jun 2018 00:00:00 +0800</pubDate>
      
      <guid>/post/topk-in-stream/</guid>
      <description>

&lt;p&gt;Before diving into Filtered-Space Saving algorithm, we will talk about the Space-Saving algorithm first, it was proposed by Metwally et al. in [1]. Space-Saving underlying idea is to monitor only a pre-defined number of m elements and their associated counters. Counters on each element are updated to reflect the maximum possible number of times an element has been observed and the error that might be involved in that estimate. If an element that is already being monitored occurs again, the counter for the frequency estimate is incremented. If the element is not currently monitored it is always added to the list. If the maximumnumber of elements has been reached, the element with the lower estimate of possible occurences is dropped. The new element estimate error is set to the estimate of frequency of the dropped element. The new element frequency estimate equal to the error plus 1.&lt;/p&gt;

&lt;p&gt;The Space-Saving algorithm will keep in the list all the elements that may have occurred at least the new estimate error value (or the last dropped element estimate) of times. This ensures that no false negatives are generated but allows for false positives. Elements with low frequencies that are observed in the end of the data stream have higher probabilities of being present in the list.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.l2f.inesc-id.pt/~fmmb/wiki/uploads/Work/dict.refd.pdf&#34; target=&#34;_blank&#34;&gt;Filtered-Space Saving&lt;/a&gt; (FSS), that uses a filtering approach to improve on Space-Saving is a data structure and algorithm combination useful for accurately estimating the top k most frequent values appearing in a stream while using a constant, minimal memory footprint. The obvious approach to computing top-k is to simply keep a table of values and their associated frequencies, which is not practical for streams.&lt;/p&gt;

&lt;p&gt;Instead, FSS works by hashing incoming values into buckets, where each bucket has a collection of values already added. If the incoming element already exists at a given bucket, its frequency is incremented. If the element doesn’t exist, it will be added as long as a few certain configurable conditions are met.&lt;/p&gt;

&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;A. Metwally, D. Agrawal, A. Abbadi, &lt;a href=&#34;http://www.cse.ust.hk/~raywong/comp5331/References/EfficientComputationOfFrequentAndTop-kElementsInDataStreams.pdf&#34; target=&#34;_blank&#34;&gt;Efficient Computation of Frequent and Top-k Elements in Data Streams&lt;/a&gt;, &lt;em&gt;Technical Report 2005-23, University of
California, Santa Barbara&lt;/em&gt;, 2005.&lt;/li&gt;
&lt;li&gt;Homem, Nuno, and J. P. Carvalho. &lt;a href=&#34;http://www.l2f.inesc-id.pt/~fmmb/wiki/uploads/Work/dict.refd.pdf&#34; target=&#34;_blank&#34;&gt;Finding top-k elements in data streams&lt;/a&gt;, &lt;em&gt;Elsevier Science Inc.&lt;/em&gt;, 2010.&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>pullword: Unsupervised Word Discovery</title>
      <link>/project/pullword/</link>
      <pubDate>Sat, 16 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/project/pullword/</guid>
      <description>

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;With the growing availability of digitized text data, there is a great need for effective computational tools to automatically extract kownledge from texts.&lt;/p&gt;

&lt;p&gt;The Chinese language differs most significantly from alphabet-based languages in not specifying word boundaries, most existing Chinese text-mining methods require a prespecified vocabulary and/or a large relevant training corpus, which may not be available in some applications.&lt;/p&gt;

&lt;p&gt;Pullword is developed for word discovering from small/large volumes of unstructured Chinese texts, and propose ways to order discovered words and conduct higher-level context analyses and it is particularly useful for mining online and domain-specific texts where the underlying vocabulary is unknown or the texts of interest differ significantly from available training corpora.&lt;/p&gt;

&lt;h2 id=&#34;implementations&#34;&gt;Implementations&lt;/h2&gt;

&lt;p&gt;The implementations mainly follow this &lt;a href=&#34;http://www.matrix67.com/blog/archives/5044&#34; target=&#34;_blank&#34;&gt;post&lt;/a&gt;.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/liuzl/pullword&#34; target=&#34;_blank&#34;&gt;golang version&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/nlpclub/nlpclub.github.io/blob/master/pullword/js/pullword.js&#34; target=&#34;_blank&#34;&gt;javascript version&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;online-demo&#34;&gt;Online demo&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://nlpclub.github.io/pullword/&#34; target=&#34;_blank&#34;&gt;javascript demo&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>q</title>
      <link>/q/</link>
      <pubDate>Thu, 07 Jun 2018 10:00:00 +0000</pubDate>
      
      <guid>/q/</guid>
      <description></description>
    </item>
    
    <item>
      <title>filestore</title>
      <link>/filestore/</link>
      <pubDate>Wed, 23 May 2018 10:00:00 +0000</pubDate>
      
      <guid>/filestore/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Gödel’s First Incompleteness Theorem for Programmers</title>
      <link>/post/godels-first-incompleteness-theorem/</link>
      <pubDate>Sun, 08 Apr 2018 00:00:00 +0800</pubDate>
      
      <guid>/post/godels-first-incompleteness-theorem/</guid>
      <description>

&lt;p&gt;Gödel’s incompleteness theorems have been hailed as “the greatest mathematical discoveries of the 20th century” — indeed, the theorems apply not only to mathematics, but all formal systems and have deep implications for science, logic, computer science, philosophy, and so on. In this post, I’ll give a simple but rigorous sketch of Gödel’s First Incompleteness Theorem. Formally, it states that:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Any consistent formal system $S$ within which a “certain amount of elementary arithmetic” can be carried out is incomplete; i.e., there are statements of the language of $S$ which can neither be proved nor disproved in $S$.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This is a rigorous proof with a focus on software engineers and programmers. I based this post on this &lt;a href=&#34;https://www.youtube.com/watch?v=9JeIG_CsgvI&#34; target=&#34;_blank&#34;&gt;excellent lecture&lt;/a&gt; I found on YouTube a little while ago. When I first learned how to prove Gödel’s incompleteness theorems, it was in the context of a metalogic class that dealt with all kinds of confusing and deep topics like Henkin construction and transfinitary logic. But as it turns out, Gödel can be understood without much fanfare!&lt;/p&gt;

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;We start on this journey by defining a couple of things. First, we define $F$ as a function that takes a positive integer and returns either &lt;code&gt;0&lt;/code&gt; or &lt;code&gt;1&lt;/code&gt;. Here’s an example of such an $F$:&lt;/p&gt;

&lt;p&gt;$$
isOdd(x)=\begin{cases}
0 &amp;amp; x\text{ is even}\\
1 &amp;amp; x\text{ is odd}
\end{cases}
$$&lt;/p&gt;

&lt;p&gt;So, we see that $isOdd(2)=0$ or that $isOdd(38943981)=1$. We can define $F$ however we want — as long as the output will either be a &lt;code&gt;0&lt;/code&gt; or a &lt;code&gt;1&lt;/code&gt;. Let $Q$ be the set of all such functions $F$.&lt;/p&gt;

&lt;p&gt;We say that $F$ is computable if there exists a computer program (or proof) $P$ that takes as input $x$ and returns $F(x)$. It goes without saying that $P$ must complete within finite time and must be correct. So let’s look at some code. Is $isOdd(x)$ computable?&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-js&#34;&gt;function isOdd(x) {
  return (x % 2);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Looks like it! This program will always return &lt;code&gt;0&lt;/code&gt; when $x$ is even and &lt;code&gt;1&lt;/code&gt; when $x$ is odd. What about a more complicated example:&lt;/p&gt;

&lt;p&gt;$$
isPrime(x)=\begin{cases}
0 &amp;amp; x\text{ is not prime}\\
1 &amp;amp; x\text{ is prime}
\end{cases}
$$&lt;/p&gt;

&lt;p&gt;Is $isPrime$ computable?&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-js&#34;&gt;function isPrime(x) {
  if(x &amp;lt; 2) return 0;
  if(x == 2) return 1;
  for(var i = 2; i &amp;lt; x; i++) {
    if(x % i === 0) return 0;
  }
  return 1;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Courtesy of this &lt;a href=&#34;https://stackoverflow.com/a/48356406/243613&#34; target=&#34;_blank&#34;&gt;Stack Overflow answer&lt;/a&gt;, it looks like it is. Let $A$ be the set of all computable functions in $Q$. We just found out (the hard way) that $isOdd$ and $isPrime$ are in $A$ — that is, they are computable.&lt;/p&gt;

&lt;p&gt;But here’s the big question: are all functions in $Q$ computable? Or, equivalently:&lt;/p&gt;

&lt;p&gt;$$
A\stackrel{?}{=}Q
$$&lt;/p&gt;

&lt;p&gt;If $A=Q$, then Gödel was wrong, so we need to figure out a clever way to show that $A\subset Q$. In other words, we need to show that there are some functions that take positive integers $x$ as input and return either a &lt;code&gt;0&lt;/code&gt; or a &lt;code&gt;1&lt;/code&gt; that we simply &lt;strong&gt;cannot implement&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&#34;the-set-up&#34;&gt;The Set-up&lt;/h2&gt;

&lt;p&gt;So how do you show that some functions $F$ are not computable? Well, let’s do it the old-fashioned way. Since we’re already using JavaScript, let’s just print out every single JavaScript program. Ever. To make things easy, we can order them alphabetically and by length (in &lt;a href=&#34;https://en.wikipedia.org/wiki/Lexicographical_order&#34; target=&#34;_blank&#34;&gt;lexicographical order&lt;/a&gt;). To make things even easier, we can just throw out programs that loop infinitely or don’t return a &lt;code&gt;1&lt;/code&gt; or a &lt;code&gt;0&lt;/code&gt;. When all is said and done, we’re left with an infinite number of programs that probably start out like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-js&#34;&gt;function F1(x) {
  return 0;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-js&#34;&gt;function F2(x) {
  return 1;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And further down the line…&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-js&#34;&gt;function Fn(x) {
  return 1 - 1;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And further down the line…&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-js&#34;&gt;function Fn(x) {
  return x / x;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And further down the line…&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-js&#34;&gt;function Fn(x) {
  return (x % 2); // hey, this is the isOdd function from before!
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And even further down the line…&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-js&#34;&gt;function Fn(x) {
  return (x % 2) / 1;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And even further down the line…&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-js&#34;&gt;function Fn(x) {
  let someRandomVariable = x ^ x;
  let abcd = someRandomVariable / y;
  if (abdc &amp;gt; -12) return 0;
  return 1;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You get the picture. Now we have every single possible program written in JavaScript that outputs &lt;code&gt;0&lt;/code&gt; or &lt;code&gt;1&lt;/code&gt;. In other words, we just populated $A$: we now have a program that goes with every single computable function. Let’s put them in a big table called $T$. The column headings indicate inputs (positive integers) and the rows indicate computable functions (and, implicitly, the programs that implement them).&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;1&lt;/th&gt;
&lt;th&gt;2&lt;/th&gt;
&lt;th&gt;3&lt;/th&gt;
&lt;th&gt;4&lt;/th&gt;
&lt;th&gt;5&lt;/th&gt;
&lt;th&gt;6&lt;/th&gt;
&lt;th&gt;7&lt;/th&gt;
&lt;th&gt;&amp;hellip;&lt;/th&gt;
&lt;th&gt;n&lt;/th&gt;
&lt;th&gt;&amp;hellip;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;$f_1$&lt;/td&gt;
&lt;td&gt;$f_1(1)$&lt;/td&gt;
&lt;td&gt;$f_1(2)$&lt;/td&gt;
&lt;td&gt;$f_1(3)$&lt;/td&gt;
&lt;td&gt;$f_1(4)$&lt;/td&gt;
&lt;td&gt;$f_1(5)$&lt;/td&gt;
&lt;td&gt;$f_1(6)$&lt;/td&gt;
&lt;td&gt;$f_1(7)$&lt;/td&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;td&gt;$f_1(n)$&lt;/td&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;$f_2$&lt;/td&gt;
&lt;td&gt;$f_2(1)$&lt;/td&gt;
&lt;td&gt;$f_2(2)$&lt;/td&gt;
&lt;td&gt;$f_2(3)$&lt;/td&gt;
&lt;td&gt;$f_2(4)$&lt;/td&gt;
&lt;td&gt;$f_2(5)$&lt;/td&gt;
&lt;td&gt;$f_2(6)$&lt;/td&gt;
&lt;td&gt;$f_2(7)$&lt;/td&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;td&gt;$f_2(n)$&lt;/td&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;$f_{o}$&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;td&gt;$f_{o}(n)$&lt;/td&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;$f_{p}$&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;td&gt;$f_{p}(n)$&lt;/td&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;$f_i$&lt;/td&gt;
&lt;td&gt;$f_i(1)$&lt;/td&gt;
&lt;td&gt;$f_i(2)$&lt;/td&gt;
&lt;td&gt;$f_i(3)$&lt;/td&gt;
&lt;td&gt;$f_i(4)$&lt;/td&gt;
&lt;td&gt;$f_i(5)$&lt;/td&gt;
&lt;td&gt;$f_i(6)$&lt;/td&gt;
&lt;td&gt;$f_i(7)$&lt;/td&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;td&gt;$f_i(n)$&lt;/td&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;You’ll notice that $isOdd$ and $isPrime$ also made it in our table (as fo and fp, respectively). So far, so good. It seems that we thought of everything. But let’s define a new function:&lt;/p&gt;

&lt;p&gt;$$
\bar{f}(i)=1-f_i(i)
$$&lt;/p&gt;

&lt;p&gt;Where $f_i$ is the $i$th function in table $T$. First, let’s make sure we’re convinced that $\bar{f}$ is well-formed. The input $i$ is an integer. $f_{i}(i)$ will return either a &lt;code&gt;0&lt;/code&gt; or a &lt;code&gt;1&lt;/code&gt;, given that $f_i$ has a row populated in $T$. And finally, $1–0= 1$ or $1–1=0$, so both cases are well-formed.&lt;/p&gt;

&lt;p&gt;$$
\bar{f}(i)=\stackrel{\text{will return }0\text{ or }1}{\overbrace{1-\underset{\text{will return 0 or 1}}{\underbrace{f_{i}(i)}}}}
$$&lt;/p&gt;

&lt;p&gt;Therefore, $\bar{f}$ is in $Q$, but is it in $T$?&lt;/p&gt;

&lt;h2 id=&#34;the-proof&#34;&gt;The Proof&lt;/h2&gt;

&lt;p&gt;Seeing why $\bar{f}$ can’t be in $T$ is pretty straightforward. Suppose $\bar{f}$ is $f_2$. Like we’ve seen so far, $f_2(2)$ can either be &lt;code&gt;0&lt;/code&gt; or &lt;code&gt;1&lt;/code&gt;. But if $f_2(2)=0$, then $\bar{f}(2)=1−f_2(2)=1−0=1$. So, we have:&lt;/p&gt;

&lt;p&gt;$$
f_2(2) \neq \bar{f}(2)
$$&lt;/p&gt;

&lt;p&gt;Whoops. Okay, so it wasn’t $f_2$. That would be too easy. What about $f_{421}$? If $f_{421}(421)=1$, then $\bar{f}(421)=1−f_{421}(421)=1−1=0$. So, we have:&lt;/p&gt;

&lt;p&gt;$$
f_{421}(421) \neq \bar{f}(421)
$$&lt;/p&gt;

&lt;p&gt;As it turns out, any computable function we pick out of $T$ (and implicitly $A$) will disagree with $\bar{f}$ at at least one output. And therefore, we have the amazing finding that:&lt;/p&gt;

&lt;p&gt;$$
A\subset Q
$$&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In other words, there are some things, like $\bar{f}$, we can’t prove or disprove in formal systems. Given that we’ve been working with JavaScript in this post, it makes sense that it’s impossible to reference the program’s own lexicographic index in $T$. With that said, you might be tempted to think that there might be a way to “get around” this limitation; if your language was clever enough, perhaps. I’ll eventually write about Gödel’s Second Incompleteness Theorem, which drives the nail in the coffin: there’s no way to get around this.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Weighted Random: algorithms for sampling from discrete probability distributions</title>
      <link>/post/weighted-random/</link>
      <pubDate>Thu, 05 Apr 2018 00:00:00 +0800</pubDate>
      
      <guid>/post/weighted-random/</guid>
      <description>

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;First of all what is weighted random? Let&amp;rsquo;s say you have a list of items and you want to pick one of them randomly. Doing this seems easy as all that&amp;rsquo;s required is to write a litte function that generates a random index referring to the one of the items in the list. But sometimes plain randomness is not enough, we want random results that are biased or based on some probability. This is where the weighted random generation algorithm needed.&lt;/p&gt;

&lt;h2 id=&#34;scenarios&#34;&gt;Scenarios&lt;/h2&gt;

&lt;p&gt;There are lots of real world scenarios that need weighted random. Such as load balancers(like &lt;a href=&#34;https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/&#34; target=&#34;_blank&#34;&gt;nginx&lt;/a&gt;, haproxy etc). Following is an example configuration of nginx. In the example, &lt;code&gt;backend1&lt;/code&gt; has weight &lt;code&gt;5&lt;/code&gt;; the other two servers have the default weight &lt;code&gt;1&lt;/code&gt;, With this configuration of weights, out of every seven requests, five are sent to &lt;code&gt;backend1&lt;/code&gt; and one to &lt;code&gt;backend2&lt;/code&gt; one to &lt;code&gt;backend3&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-nginx&#34;&gt;http {
    upstream backend {
        server backend1 weight=5;
        server backend2;
        server backend3;
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Another example is crawler scheduling. When I was developing a concurrent &lt;a href=&#34;https://github.com/crawlerclub/x&#34; target=&#34;_blank&#34;&gt;crawling framework&lt;/a&gt; last year, I need to schedule the crawling tasks according to the task importenceness. The tasks importenceness are expressed by float value weights that are mannually assigned to each site that tasks belong to. So there should be a &lt;a href=&#34;https://github.com/crawlerclub/x/blob/master/controller/crawler_scheduler.go#L45&#34; target=&#34;_blank&#34;&gt;WeightedChoice&lt;/a&gt; function on the scheduler of the crawler system that determines which task should be scheduled the next time.&lt;/p&gt;

&lt;p&gt;In the negative sampling part of the famous &lt;code&gt;word2vec&lt;/code&gt;, the algorithm needs to randomly sample some negative words according to their frequencies in the corpus. &lt;a href=&#34;https://github.com/tmikolov/word2vec/blob/master/word2vec.c#L527&#34; target=&#34;_blank&#34;&gt;Codes link&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;There are more examples in game developing: In games we often encounter random dropping of specified items by certain drop probability, such as falling silver coins 25%, gold coins 20%, diamonds 10%, equipment 5%, accessories 40%. The next dropped item type is now required to meet the above probability.&lt;/p&gt;

&lt;h2 id=&#34;solutions&#34;&gt;Solutions&lt;/h2&gt;

&lt;h3 id=&#34;solution-1&#34;&gt;Solution 1&lt;/h3&gt;

&lt;p&gt;The first method came up to me is to extend the uniform distributed random number generator. Let&amp;rsquo;s begin with an example(all example programmes here after will be writen in Golang):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;var items   = []int{0, 1, 2, 3}
var weights = []float32{0.1, 0.3, 0.4, 0.2}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Making them conform to the weights what we’d do is something simple. Basically repeat the items 10x or even 100x times based on the numbers we have. So let’s say we’re repeating 10x times, this is the list we’ll end up with:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;var choices = []int{0, 1, 1, 1, 2, 2, 2, 2, 3, 3}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then we can randomly choose a value from &lt;code&gt;choices&lt;/code&gt;, and we are done. Full codes bellow:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;package main

import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;math/rand&amp;quot;
)

func WeightedRandomS1(weights []float32) int {
    if len(weights) == 0 {
        return 0
    }
    var choices []int
    for i, w := range weights {
        wi := int(w * 10)
        for j := 0; j &amp;lt; wi; j++ {
            choices = append(choices, i)
        }
    }
    return choices[rand.Int()%len(choices)]
}

func main() {
    for i := 0; i &amp;lt; 100; i++ {
        fmt.Println(WeightedRandom([]float32{0.1, 0.3, 0.6}))
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In &lt;code&gt;word2vec&lt;/code&gt;, this solution is adopted.&lt;/p&gt;

&lt;h3 id=&#34;solution-2&#34;&gt;Solution 2&lt;/h3&gt;

&lt;p&gt;The first solution takes too much memory, then came solution 2: Compute the discrete cumulative density function (CDF) of the list &amp;ndash; or in simple terms the array of cumulative sums of the weights. Then generate a random number in the range between 0 and the sum of all weights, do a linear search to find this random number in your discrete CDF array and get the value corresponding to this entry &amp;ndash; this is the weighted random number.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func WeightedRandomS2(weights []float32) int {
    if len(weights) == 0 {
        return 0
    }
    var sum float32 = 0.0
    for _, w := range weights {
        sum += w
    }
    r := rand.Float32() * sum
    for i, w := range weights {
        r -= w
        if r &amp;lt; 0 {
            return i
        }
    }
    return len(weights) - 1
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I used this solution in the scheduler of &lt;a href=&#34;https://github.com/crawlerclub/x&#34; target=&#34;_blank&#34;&gt;crawling framework&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;solution-3&#34;&gt;Solution 3&lt;/h3&gt;

&lt;p&gt;Adopting binary search over the CDF array could reduce the time complexity from $O(n)$ to $O(log(n))$.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func WeightedRandomS3(weights []float32) int {
    n := len(weights)
    if n == 0 {
        return 0
    }
    cdf := make([]float32, n)
    var sum float32 = 0.0
    for i, w := range weights {
        if i &amp;gt; 0 {
            cdf[i] = cdf[i-1] + w
        } else {
            cdf[i] = w
        }
        sum += w
    }
    r := rand.Float32() * sum
    var l, h int = 0, n - 1
    for l &amp;lt;= h {
        m := l + (h-l)/2
        if r &amp;lt;= cdf[m] {
            if m == 0 || (m &amp;gt; 0 &amp;amp;&amp;amp; r &amp;gt; cdf[m-1]) {
                return m
            }
            h = m - 1
        } else {
            l = m + 1
        }
    }
    return -1
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;solution-4&#34;&gt;Solution 4&lt;/h3&gt;

&lt;p&gt;The optimal solution for weighted random should be the &lt;a href=&#34;https://en.wikipedia.org/wiki/Alias_method&#34; target=&#34;_blank&#34;&gt;Alias Method&lt;/a&gt;. It requires $O(n)$ time to initialize, $O(1)$ time to make a selection, and $O(n)$ memory. A golang version implementation is &lt;a href=&#34;https://github.com/liuzl/alias&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&#34;algorithm-vose-s-alias-method&#34;&gt;Algorithm: Vose&amp;rsquo;s Alias Method&lt;/h4&gt;

&lt;h5 id=&#34;initialization&#34;&gt;Initialization:&lt;/h5&gt;

&lt;ol&gt;
&lt;li&gt;Create arrays $Alias$ and $Prob$, each of size $n$.&lt;/li&gt;
&lt;li&gt;Create two worklists, $Small$ and $Large$.&lt;/li&gt;
&lt;li&gt;Multiply each probability by $n$.&lt;/li&gt;
&lt;li&gt;For each scaled probability $p_i$:

&lt;ol&gt;
&lt;li&gt;If $p_i&amp;lt;1$, add $i$ to $Small$.&lt;/li&gt;
&lt;li&gt;Otherwise $p_i \geqslant 1$, add $i$ to $Large$.&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;While $Small$ and $Large$ are not empty: ($Large$ might be emptied first)

&lt;ol&gt;
&lt;li&gt;Remove the first element from $Small$; call it $l$.&lt;/li&gt;
&lt;li&gt;Remove the first element from $Large$; call it $g$.&lt;/li&gt;
&lt;li&gt;Set $Prob[l]=p_l$.&lt;/li&gt;
&lt;li&gt;Set $Alias[l]=g$.&lt;/li&gt;
&lt;li&gt;Set $p_g = p_g + p_l - 1$. (This is a more numerically stable option)&lt;/li&gt;
&lt;li&gt;If $p_g&amp;lt;1$, add $g$ to $Small$.&lt;/li&gt;
&lt;li&gt;Otherwise $p_g \geqslant 1$, add $g$ to $Large$.&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;While $Large$ is not empty:

&lt;ol&gt;
&lt;li&gt;Remove the first element from $Large$; call it $g$.&lt;/li&gt;
&lt;li&gt;Set $Prob[g] = 1$.&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;While $Small$ is not empty: This is only possible due to numerical instability.

&lt;ol&gt;
&lt;li&gt;Remove the first element from $Small$; call it $l$.&lt;/li&gt;
&lt;li&gt;Set $Prob[l] = 1$.&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h5 id=&#34;generation&#34;&gt;Generation:&lt;/h5&gt;

&lt;ol&gt;
&lt;li&gt;Generate a fair die roll from an n-sided die; call the side $i$.&lt;/li&gt;
&lt;li&gt;Flip a biased coin that comes up heads with probability $Prob[i]$.&lt;/li&gt;
&lt;li&gt;If the coin comes up &amp;ldquo;heads&amp;rdquo;, return $i$.&lt;/li&gt;
&lt;li&gt;Otherwise, return $Alias[i]$.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Walker, A. J. 1977. &amp;ldquo;An efficient method for generating discrete random variable with general distributions.&amp;rdquo; &lt;em&gt;ACM Transactions on Mathematical Software&lt;/em&gt; &lt;strong&gt;3&lt;/strong&gt; 253–256.&lt;/li&gt;
&lt;li&gt;&amp;ldquo;&lt;a href=&#34;http://www.keithschwarz.com/darts-dice-coins/&#34; target=&#34;_blank&#34;&gt;Darts, Dice, and Coins: Sampling from a Discrete Distribution&lt;/a&gt;&amp;rdquo;. &lt;em&gt;Keith Schwarz&lt;/em&gt;, December 29, 2011&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Alias_method&#34; target=&#34;_blank&#34;&gt;Alias method&lt;/a&gt;. &lt;em&gt;Wikipedia&lt;/em&gt;, April 5, 2018&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://stackoverflow.com/questions/4463561/weighted-random-selection-from-array&#34; target=&#34;_blank&#34;&gt;Weighted random selection from array
&lt;/a&gt;, &lt;em&gt;stackoverflow&lt;/em&gt;, Dec 16 2010&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>FMR: functional meaning representation</title>
      <link>/project/fmr/</link>
      <pubDate>Fri, 16 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/project/fmr/</guid>
      <description>

&lt;h2 id=&#34;element-types-in-fmr&#34;&gt;Element types in FMR&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Constants&lt;/strong&gt;: Refer to specific objects in the world. A constant can be a number, a lexical string, or an entity.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Classes&lt;/strong&gt;: Semantic category of entities. For example: &lt;code&gt;location.city&lt;/code&gt;, &lt;code&gt;math.number&lt;/code&gt;

&lt;ul&gt;
&lt;li&gt;Sub-class: &lt;em&gt;math.number ⊆ math.expression&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Template classes: classes with one or more parameters, for example: &lt;code&gt;t.list&amp;lt;c, m, n&amp;gt;&lt;/code&gt;, where &lt;code&gt;c&lt;/code&gt; is a class, &lt;code&gt;m&lt;/code&gt; and &lt;code&gt;n&lt;/code&gt; are integers.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Functions&lt;/strong&gt;: The major way to form larger language units from smaller ones. A function is comprised of a &lt;em&gt;name&lt;/em&gt;, a list of &lt;em&gt;core arguments&lt;/em&gt;, and a &lt;em&gt;return type&lt;/em&gt;.

&lt;ul&gt;
&lt;li&gt;Noun functions

&lt;ul&gt;
&lt;li&gt;Map entities to their properties or to other entities having specific relations withe the argument(s).&lt;/li&gt;
&lt;li&gt;Are used to represent noun phrases in natural language.&lt;/li&gt;
&lt;li&gt;Pronoun functions are special zero-argument noun functions.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Verb functions

&lt;ul&gt;
&lt;li&gt;Act as sentences or sub-sentences&lt;/li&gt;
&lt;li&gt;The most important function type?&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Modifier functions

&lt;ul&gt;
&lt;li&gt;Many functions can take additional &lt;em&gt;extended arguments&lt;/em&gt; as their modifiers.&lt;/li&gt;
&lt;li&gt;Modifier functions often take the role of extended arguments to modify noun function, verb functions or other modifier functions.&lt;/li&gt;
&lt;li&gt;Are used in FMR as the semantic representation of adjectives, adverb phrases (including conjunctive adverb phrases), and prepositional phrases in NL&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Entity Variables&lt;/strong&gt;: Variables are assigned to FMR nodes for indicating the co-reference of sub-trees to entities.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;features-of-fmr&#34;&gt;Features of FMR&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Strongly typed language

&lt;ul&gt;
&lt;li&gt;Type-compatibility: The type of each child of a function node should match the corresponding argument type of the function.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Open-domain type system.

&lt;ul&gt;
&lt;li&gt;Thousands of types&lt;/li&gt;
&lt;li&gt;Other languages: At most 100+ in grammar level&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Built-in data structures

&lt;ul&gt;
&lt;li&gt;like &lt;code&gt;t.list&lt;/code&gt; and &lt;code&gt;nf.list&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;semantic-parsing-from-nl-to-fmr&#34;&gt;Semantic parsing from NL to FMR&lt;/h2&gt;

&lt;p&gt;By using CFG rules to map natural language sentences and phrases to FMR&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;&amp;quot;The product of 3 consecutive number is 60&amp;quot; =&amp;gt; 
vf.be.equ(nf.math.product(nf.list(math.number, 3, mf.consecutive)), 60);
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;缘起&#34;&gt;缘起&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;语义理解平台/语音助手（2013-今，腾讯/搜狗）

&lt;ul&gt;
&lt;li&gt;复合实体识别（CRF识别/规则识别/词表识别）&lt;/li&gt;
&lt;li&gt;基于语言模型的查询意图理解&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;基于语义模板的查询意图识别和语义信息抽取&lt;/strong&gt;

&lt;ul&gt;
&lt;li&gt;通用CFG文法解析器：高效Earley算法的实现&lt;/li&gt;
&lt;li&gt;意图识别，槽位填充&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;查询意图排序，基于候选意图结果的终判模块&lt;/li&gt;
&lt;li&gt;下游分类别知识库系统/垂直搜索系统&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>gocc: Golang version OpenCC 繁簡轉換</title>
      <link>/project/gocc/</link>
      <pubDate>Fri, 16 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/project/gocc/</guid>
      <description>

&lt;h2 id=&#34;introduction-介紹&#34;&gt;Introduction 介紹&lt;/h2&gt;

&lt;p&gt;gocc is a golang port of OpenCC(&lt;a href=&#34;https://github.com/BYVoid/OpenCC/&#34; target=&#34;_blank&#34;&gt;Open Chinese Convert 開放中文轉換&lt;/a&gt;) which is a project for conversion between Traditional and Simplified Chinese developed by &lt;a href=&#34;https://www.byvoid.com/&#34; target=&#34;_blank&#34;&gt;BYVoid&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;gocc stands for &amp;ldquo;&lt;strong&gt;Go&lt;/strong&gt;lang version Open&lt;strong&gt;CC&lt;/strong&gt;&amp;rdquo;, it is a total rewrite version of OpenCC by Go. It just borrows the dict files and config files of OpenCC, so it may not produce the same output with the original OpenCC.&lt;/p&gt;

&lt;h2 id=&#34;installation-安裝&#34;&gt;Installation 安裝&lt;/h2&gt;

&lt;h3 id=&#34;1-golang-package&#34;&gt;1, golang package&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;go get github.com/liuzl/gocc
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;2-command-line&#34;&gt;2, Command Line&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git clone https://github.com/liuzl/gocc
cd gocc/cmd
make install
gocc --help
echo &amp;quot;我们是工农子弟兵&amp;quot; | gocc
#我們是工農子弟兵
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;usage-使用&#34;&gt;Usage 使用&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;package main

import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;github.com/liuzl/gocc&amp;quot;
    &amp;quot;log&amp;quot;
)

func main() {
    s2t, err := gocc.New(&amp;quot;s2t&amp;quot;)
    if err != nil {
        log.Fatal(err)
    }
    in := `自然语言处理是人工智能领域中的一个重要方向。`
    out, err := s2t.Convert(in)
    if err != nil {
        log.Fatal(err)
    }
    fmt.Printf(&amp;quot;%s\n%s\n&amp;quot;, in, out)
    //自然语言处理是人工智能领域中的一个重要方向。
    //自然語言處理是人工智能領域中的一個重要方向。
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;conversions&#34;&gt;Conversions&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;s2t&lt;/code&gt; Simplified Chinese to Traditional Chinese&lt;/li&gt;
&lt;li&gt;&lt;code&gt;t2s&lt;/code&gt; Traditional Chinese to Simplified Chinese&lt;/li&gt;
&lt;li&gt;&lt;code&gt;s2tw&lt;/code&gt; Simplified Chinese to Traditional Chinese (Taiwan Standard)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tw2s&lt;/code&gt; Traditional Chinese (Taiwan Standard) to Simplified Chinese&lt;/li&gt;
&lt;li&gt;&lt;code&gt;s2hk&lt;/code&gt; Simplified Chinese to Traditional Chinese (Hong Kong Standard)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;hk2s&lt;/code&gt; Traditional Chinese (Hong Kong Standard) to Simplified Chinese&lt;/li&gt;
&lt;li&gt;&lt;code&gt;s2twp&lt;/code&gt; Simplified Chinese to Traditional Chinese (Taiwan Standard) with Taiwanese idiom&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tw2sp&lt;/code&gt; Traditional Chinese (Taiwan Standard) to Simplified Chinese with Mainland Chinese idiom&lt;/li&gt;
&lt;li&gt;&lt;code&gt;t2tw&lt;/code&gt; Traditional Chinese (OpenCC Standard) to Taiwan Standard&lt;/li&gt;
&lt;li&gt;&lt;code&gt;t2hk&lt;/code&gt; Traditional Chinese (OpenCC Standard) to Hong Kong Standard&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>goutil</title>
      <link>/goutil/</link>
      <pubDate>Mon, 20 Nov 2017 10:00:00 +0000</pubDate>
      
      <guid>/goutil/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Illustration of the logistic map</title>
      <link>/post/logistic-map/</link>
      <pubDate>Thu, 15 Jun 2017 00:00:00 +0800</pubDate>
      
      <guid>/post/logistic-map/</guid>
      <description>&lt;p&gt;Illustration of the logistic map, $x_{n+1} = f(x_n) = r \cdot x_n \cdot (1 - x_n)$&lt;/p&gt;

&lt;p&gt;At $r_1=3$, a stable period $2^1=2$ orbit is born. At $r_2=3.449$ a stable period $2^2=4$ orbit is born. As $r$ continues to increase, the period doublings continue until $r_{\infty} \approx 3.56995$ after which chaotic dynamics begin to occur, interspersed with periodic windows.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&#34;http://en.wikipedia.org/wiki/Feigenbaum_constants&#34; target=&#34;_blank&#34;&gt;Feigeinbaum constant&lt;/a&gt; $\delta=4.6692\dots$ is the ratio of subsequent differences between the values of $r_n$ at which the period doubles, as $n$ approaches infinity.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np
import matplotlib.pyplot as plt
import pylab

def f(x, R):
    return R * x * (1 - x)

def run_simulation(R, x_0, num_steps):
    x_list = np.zeros(num_steps)
    x_list[0] = x_0   
    for t in range(num_steps-1):
        x_list[t+1] = f(x_list[t], R)       
    return x_list

def plot_two(x_list, y_list):
    plt.plot(x_list)
    plt.plot(y_list)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x_list = run_simulation(R=4, x_0=0.7, num_steps=50)
y_list = run_simulation(R=4, x_0=0.70001, num_steps=50)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plot_two(x_list, y_list)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;/img/logistic_map_3_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ling: A Natural Language Processing toolkit in Golang</title>
      <link>/project/ling/</link>
      <pubDate>Thu, 27 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>/project/ling/</guid>
      <description>

&lt;h2 id=&#34;other-nlp-tools&#34;&gt;other NLP tools&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://stanfordnlp.github.io/CoreNLP/index.html&#34; target=&#34;_blank&#34;&gt;Stanford CoreNLP&lt;/a&gt; Java&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://spacy.io/&#34; target=&#34;_blank&#34;&gt;spaCy&lt;/a&gt; Python&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/chewxy/lingo&#34; target=&#34;_blank&#34;&gt;lingo&lt;/a&gt; Golang&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;multilingual-text-toknization&#34;&gt;Multilingual text toknization&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.unicode.org/reports/tr29/&#34; target=&#34;_blank&#34;&gt;Unicode Standard Annex #29&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/liuzl/segment&#34; target=&#34;_blank&#34;&gt;blevesearch segment&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;text-normalization&#34;&gt;Text normalization&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.golang.org/normalization&#34; target=&#34;_blank&#34;&gt;Text normalization in Go&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;lemmatization&#34;&gt;Lemmatization&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;词干提取（stemming）和词形还原（lemmatization）&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html&#34; target=&#34;_blank&#34;&gt;Stemming and lemmatization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.lexiconista.com/datasets/lemmatization/&#34; target=&#34;_blank&#34;&gt;Lemmatization Lists&lt;/a&gt;&lt;em&gt;&lt;sub&gt;Datasets by MBM &lt;/sub&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://unimorph.github.io/&#34; target=&#34;_blank&#34;&gt;The UniMorph Project&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;中文繁简转换

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/liuzl/gocc&#34; target=&#34;_blank&#34;&gt;gocc&lt;/a&gt; Golang version OpenCC&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/BYVoid/OpenCC&#34; target=&#34;_blank&#34;&gt;OpenCC&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/go-cc/cc-jianfan&#34; target=&#34;_blank&#34;&gt;Chinese-Character Jian&amp;lt;=&amp;gt;Fan converting library in Go&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/siongui/gojianfan&#34; target=&#34;_blank&#34;&gt;Traditional and Simplified Chinese Conversion in Go&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Han_unification&#34; target=&#34;_blank&#34;&gt;Han unification&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;tagging&#34;&gt;Tagging&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Regex tagger

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/mingrammer/commonregex&#34; target=&#34;_blank&#34;&gt;commonregex&lt;/a&gt;, a collection of common regular expressions for Go.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/mvdan/xurls&#34; target=&#34;_blank&#34;&gt;xurls&lt;/a&gt;, a Go package of regex for urls.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>『大数据』方法论及示例</title>
      <link>/talk/bigdata-cufe/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0800</pubDate>
      
      <guid>/talk/bigdata-cufe/</guid>
      <description></description>
    </item>
    
    <item>
      <title>人工智能的过去、现在和未来</title>
      <link>/talk/ai-sif100/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0800</pubDate>
      
      <guid>/talk/ai-sif100/</guid>
      <description></description>
    </item>
    
    <item>
      <title>大数据的“能”与“不能”</title>
      <link>/talk/bigdata-ether-n5/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0800</pubDate>
      
      <guid>/talk/bigdata-ether-n5/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
