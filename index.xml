<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Zhanliang Liu on Zhanliang Liu</title>
    <link>/</link>
    <description>Recent content in Zhanliang Liu on Zhanliang Liu</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2020</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 +0800</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Language Model Is All You Need‚Ää‚Äî‚ÄäExplores NLU as MRC QA</title>
      <link>/post/qanlu-language-model-is-all-you-need/</link>
      <pubDate>Tue, 12 Jan 2021 00:00:00 +0800</pubDate>
      
      <guid>/post/qanlu-language-model-is-all-you-need/</guid>
      <description>&lt;p&gt;New research from Amazon Alexa AI posits that current natural language understanding (NLU) approaches are far from how humans understand language, and asks whether all NLU problems could be efficiently and effectively mapped to question-answering (QA) problems using transfer learning.&lt;/p&gt;

&lt;p&gt;Transfer learning is an ML approach for applying knowledge learned from a source domain to a target domain. It has produced promising results in natural language processing (NLP), particularly when transferring learning from high data domains to low data domains. The Amazon researchers focus on a specific type of transfer learning, where the target domain is first mapped to the source domain.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/qanlu_fg1.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;NLU is taken as determining intent and slot or entity value in natural language utterances. The proposed &amp;ldquo;QANLU&amp;rdquo; approach builds slot and intent detection questions and answers based on NLU annotated data. QA models are first trained on QA corpora the fine-tuned on questions and answers created from the NLU annotated data. Through transfer learning, this contextual question-answering knowledage is then used for finding intents or slot values in text inputs.&lt;/p&gt;

&lt;p&gt;Unlike previous approaches, QANLU focuses on how low resource applications and does not the design and training of new model architectures or extensive data preprocessing. This enables it to achieve strong results in slot and intent detection with an order of magnitude less data.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/qanlu_fg2.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The researchers conducted experiments on the ATIS and Restaurants-8k datasets, with QANLU in low data regimes and few-shot settings significantly outperforming sentence classification and token tagging approaches for intent and slot detection tasks, while also bettering the new IC/SF few-shot approach&amp;rsquo;s performance in NLU.&lt;/p&gt;

&lt;p&gt;The researchers say future directions could include expanding beyond this configuration and across different NLP problems, measuring the transfer of knowledge across different NLP tasks, and studying how QANLU questions might be generated automatically based on context.&lt;/p&gt;

&lt;p&gt;The paper &lt;em&gt;Language Model Is All You Need: Natural Language Understanding as Question Answering&lt;/em&gt; is on &lt;a href=&#34;https://arxiv.org/pdf/2011.03023.pdf&#34; target=&#34;_blank&#34;&gt;arXiv&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>‰ªéAIÂà∞IAÔºåÁü•ËØÜÂõæË∞±‰ø°ÊÅØÊäΩÂèñÂú®Êô∫ËÉΩÂÆ°Âçï‰∏öÂä°‰∏≠ÁöÑÂ∫îÁî®ÂÆûË∑µ</title>
      <link>/talk/aiprocon2020/</link>
      <pubDate>Tue, 10 Nov 2020 00:00:00 +0800</pubDate>
      
      <guid>/talk/aiprocon2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ‰∏éË°å‰∏öÂ∫îÁî®</title>
      <link>/talk/waidc2019/</link>
      <pubDate>Sat, 16 Nov 2019 00:00:00 +0800</pubDate>
      
      <guid>/talk/waidc2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>ËØ≠‰πâËß£ÊûêÂú®Êô∫ËÉΩÊêúÁ¥¢ÂèäAIË°å‰∏öËêΩÂú∞Á≠âÊñπÂêëÁöÑÂ∫îÁî®</title>
      <link>/talk/ccir2019/</link>
      <pubDate>Sun, 22 Sep 2019 00:00:00 +0800</pubDate>
      
      <guid>/talk/ccir2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>ds</title>
      <link>/ds/</link>
      <pubDate>Sat, 20 Jul 2019 10:00:00 +0000</pubDate>
      
      <guid>/ds/</guid>
      <description></description>
    </item>
    
    <item>
      <title>‚õµ Learning Meaning in Natural Language Processing‚Ää‚Äî‚ÄäThe Semantics Mega-Thread</title>
      <link>/post/semantic-meaning/</link>
      <pubDate>Thu, 15 Nov 2018 00:00:00 +0800</pubDate>
      
      <guid>/post/semantic-meaning/</guid>
      <description>

&lt;p&gt;I found a very worthwhile article while surfing medium.com days ago. The article is a summary of a twitter thread which talked about meaning, semantics, language models, learning Thai and Java, entailment, co-reference‚Ää‚Äî‚Ääall in one fascinating thread. The original article is &lt;a href=&#34;https://medium.com/huggingface/learning-meaning-in-natural-language-processing-the-semantics-mega-thread-9c0332dfe28e&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Following is a copy of the original article:&lt;/p&gt;

&lt;p&gt;Last week &lt;a href=&#34;https://twitter.com/jacobandreas/status/1023246560082063366&#34; target=&#34;_blank&#34;&gt;a tweet by Jacob Andreas&lt;/a&gt; triggered a huge discussion on Twitter that many people have called the &lt;em&gt;meaning/semantics mega-thread&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id=&#34;a-crash-course-on-lexical-meaning-semantics&#34;&gt;üèé A crash course on Lexical Meaning &amp;amp; Semantics&lt;/h2&gt;

&lt;p&gt;If you already know what we mean by &lt;em&gt;‚ÄúMeaning/Semantics‚Äù&lt;/em&gt; in NLP, you can skip this part and go straight to the debate üî•.&lt;/p&gt;

&lt;p&gt;For the CS/ML folks out there here are a few words of introduction.&lt;/p&gt;

&lt;p&gt;First, it‚Äôs important to state that Meaning in Natural Language is a multi-facetted concept with semantic, pragmatic, cognitive and social aspects. The discussion that happened on Twitter was mainly about lexical semantics and compositionality so I will focus on this sub-field for brevity. You will find additional links to broaden this view at the end of this section.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Meaning_%28linguistics%29&#34; target=&#34;_blank&#34;&gt;Meaning&lt;/a&gt; is the information that a sender intends to convey, or does convey, to a receiver.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Now, we know that &lt;strong&gt;strings are already a representation of meaning&lt;/strong&gt;, so why should we go any further than just raw text?&lt;/p&gt;

&lt;p&gt;Well there are several reasons we may want to distinguish meaning from raw text.&lt;/p&gt;

&lt;p&gt;One reason is that the field of NLP/NLU aims at &lt;strong&gt;building systems&lt;/strong&gt; that understand what you say to them, trigger actions based on that and convey back meaningful information. Let‚Äôs take a simple example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Context: Knowledge of mathematics
Utterance: What is the largest prime less than 10?
Action: 7
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Given some knowledge of math, we want our NLU system to produce an appropriate answer.&lt;/p&gt;

&lt;p&gt;It‚Äôs difficult to (i) link raw text to a knowledge base of mathematical facts in our system and (ii) combine pieces of knowledge together to infer an answer. One solution is to define an intermediate meaning representation (sometimes called a Logical Form) that is more easy to manipulate.&lt;/p&gt;

&lt;p&gt;For example in our case:&lt;/p&gt;

&lt;p&gt;$$
Meaning = max(primes \cap (-\infty, 10))
$$&lt;/p&gt;

&lt;p&gt;We can then execute this expression with respect to a model of the world, like our database of knowledge, to get an answer. This way, we have also factored out the understanding of language (called &lt;em&gt;semantic parsing&lt;/em&gt;) from the world knowledge (the problem of grounding meaning of in the real word).&lt;/p&gt;

&lt;p&gt;Advantageously, our &lt;em&gt;representation of the meaning of a sentence&lt;/em&gt; can thus:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;provide a way to &lt;strong&gt;link language&lt;/strong&gt; to external &lt;strong&gt;knowledge base&lt;/strong&gt;, &lt;strong&gt;observations&lt;/strong&gt;, and &lt;strong&gt;actions&lt;/strong&gt;;&lt;/li&gt;
&lt;li&gt;support &lt;strong&gt;computational inference&lt;/strong&gt;, so that concepts can be &lt;strong&gt;combined&lt;/strong&gt; to derive additional knowledge as human do during a conversation.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Two other nice requirements for this representation:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;unambiguous&lt;/strong&gt;: one meaning per statement (unlike natural language);&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;expressive&lt;/strong&gt; enough to cover the full range of things that people talk about.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Natural language as raw text doesn‚Äôt fulfill most of these criteria!&lt;/p&gt;

&lt;p&gt;A related line of research is &lt;strong&gt;Formal Semantics&lt;/strong&gt; which seek to understand linguistic meaning by constructing models of the principles that speakers use to convey meaning.&lt;/p&gt;

&lt;p&gt;The tools of formal semantics are similar to NLU/NLP tools but the aim is to understand how people construct meaning more than any specific application.&lt;/p&gt;

&lt;p&gt;Now there is a lot more to meaning than just logic forms and grounding. A few examples: &lt;strong&gt;‚ÄúBut I didn‚Äôt mean it literally!!‚Äù&lt;/strong&gt; (speaker meaning ‚â† literal meaning), &lt;strong&gt;‚ÄúBuffalo buffalo Buffalo buffalo buffalo buffalo Buffalo buffalo.‚Äù&lt;/strong&gt; (yes, this is a real sentence with a meaning but you need to find the right sense for each buffalo!) and so on&amp;hellip;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;A few pointers: Our simple example came from &lt;a href=&#34;https://cs.stanford.edu/~pliang/papers/executable-cacm2016.pdf&#34; target=&#34;_blank&#34;&gt;this nice article by Percy Liang&lt;/a&gt;. As a quick overview of the field, I would recommend chapters 12 and 13 of J. Eisenstein‚Äôs book &lt;a href=&#34;https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf&#34; target=&#34;_blank&#34;&gt;‚ÄúNatural Language Processing‚Äù&lt;/a&gt;. They will take you through the main ideas, tools up to recent research on Meaning in NLP. &lt;a href=&#34;http://faculty.washington.edu/ebender/100things-sem_prag.html&#34; target=&#34;_blank&#34;&gt;Emily M. Bender‚Äôs ACL 2018 tutorial&lt;/a&gt; is a nice way to see how Meaning can be a multi-headed monster üêç to say the least!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Now back to our mega-thread!&lt;/p&gt;

&lt;h2 id=&#34;triggering-a-debate-on-meaning&#34;&gt;üî• Triggering a debate on Meaning&lt;/h2&gt;

&lt;p&gt;As often, the discussion was sparked by a mention of sentence embeddings.&lt;/p&gt;

&lt;p&gt;This argument was the main trigger for the mega-discussion that followed. Further in the thread, Emily M. Bender reformulates her argument as:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;If all the learner gets is text the learner cannot learn meaning.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;can-a-model-trained-only-on-raw-text-learn-meaning&#34;&gt;Can a model trained only on raw text learn meaning?&lt;/h2&gt;

&lt;p&gt;This question was explored along two axes:&lt;/p&gt;

&lt;h3 id=&#34;what-aspect-of-meaning-can-a-model-learn&#34;&gt;What aspect of Meaning can a model learn?&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Can it learn &lt;em&gt;meaning&lt;/em&gt; or just learn &lt;em&gt;similarity&lt;/em&gt; in meaning (i.e. learn that some expressions are similar without knowing what they mean. It is still very useful for transfer learning)?&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Can it learn &lt;em&gt;grounded meaning&lt;/em&gt; (learn the meaning of each expression as a state of the world state) or learn &lt;em&gt;lexical meaning&lt;/em&gt; (e.g. learn how the meaning of sub-expressions compose together, as in our logical forms)?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;how-can-the-model-learn&#34;&gt;How can the model Learn?&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;If the model cannot learn meaning from raw text alone, what would be the &lt;em&gt;minimal amount of additional supervision&lt;/em&gt; needed? Should we add supervision from Logical Forms, Textual Entailment&amp;hellip;?&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Could we encode some &lt;em&gt;inductive bias&lt;/em&gt; in the model so that it can learn aspects of meaning from raw text?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;the-thai-and-java-experiments&#34;&gt;The Thai and Java Experiments&lt;/h2&gt;

&lt;p&gt;Emily M. Bender proposed several interesting experiments which were discussed at length:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://twitter.com/emilymbender/status/1024042044035985408&#34; target=&#34;_blank&#34;&gt;The Thai Room experiment&lt;/a&gt;: &lt;em&gt;‚ÄúImagine [you] were given the sum total of all Thai literature in a huge library. (All in Thai, no translations.) Assuming you don‚Äôt already know Thai, you won‚Äôt learn it from that.‚Äù&lt;/em&gt; &lt;strong&gt;A real life example of trying to learn from raw text only.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://twitter.com/emilymbender/status/1025002835467890689&#34; target=&#34;_blank&#34;&gt;The Java Code experiment&lt;/a&gt;: &lt;em&gt;‚ÄúGive your NN all well-formed java code that‚Äôs ever been written, but only the surface form of the code. Then ask it to evaluate (i.e. execute) part of it.‚Äù&lt;/em&gt; &lt;strong&gt;Can we learn execution semantics from raw text only?&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;investigating-programming-language-semantic&#34;&gt;Investigating Programming Language Semantic&lt;/h2&gt;

&lt;p&gt;The Java Code proposal triggered an interesting discussion on the difference between trying to learn meaning from Programming Language (PL) code and from Natural Language (NL) text.&lt;/p&gt;

&lt;p&gt;It actually seems more difficult to learn from PL than NL.&lt;/p&gt;

&lt;p&gt;Learning meaning from Java code is like having a text composed only of orders/commands and without any descriptions. But description are very important feedbacks for learning as they allow one to compare its internal world state with the real world state.&lt;/p&gt;

&lt;h2 id=&#34;language-models&#34;&gt;Language Models&lt;/h2&gt;

&lt;p&gt;The discussion circled around Language Models. A language model is a model which can &lt;strong&gt;predict the next word in a sentence given a context&lt;/strong&gt; (past words, external knowledge). Recently, these models have given &lt;a href=&#34;https://blog.openai.com/language-unsupervised/&#34; target=&#34;_blank&#34;&gt;interesting results&lt;/a&gt; in commonsense reasoning. Language models were examined in two settings:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Independently of any training dataset&lt;/strong&gt;: having a &lt;em&gt;human-level language model&lt;/em&gt; involves that the model has a human-like notion of meaning and world state. As &lt;a href=&#34;https://twitter.com/yoavgo/status/1025485692347056128&#34; target=&#34;_blank&#34;&gt;Yolav Goldberg mentioned&lt;/a&gt; &lt;em&gt;‚Äúit‚Äôs just (one of the many) trivial examples where perfect LM entails solving not only all of language but also all of AI.‚Äù&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;More interesting is the case of a language model &lt;strong&gt;trained from raw text only&lt;/strong&gt;: here the question is how much the model can learn in terms of semantics without being given access to explicit meaning information!&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;textual-entailment&#34;&gt;Textual Entailment&lt;/h2&gt;

&lt;p&gt;One way to give some information about meaning without departing too much from raw text is to train a model on Textual Entailment, the task of &lt;strong&gt;predicting whether a sentence imply another&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;In a series tweets, Sam Bowman explained his view that entailment could be used to learn ‚Äúthe meat of compositional‚Äù semantics almost from raw text.&lt;/p&gt;

&lt;p&gt;And also made the suggestion that a learner might be able to learn entailment in a simpler way than curent setups, using a setup that could be close to LM.&lt;/p&gt;

&lt;h2 id=&#34;an-inductive-bias-language-model&#34;&gt;An Inductive Bias Language Model&lt;/h2&gt;

&lt;p&gt;Another way to learn meaning from a dataset as close as possible to raw text is to put a strong inductive bias in the model as discussed by Matt Gardner.&lt;/p&gt;

&lt;p&gt;One example is the &lt;a href=&#34;https://arxiv.org/abs/1708.00781&#34; target=&#34;_blank&#34;&gt;Entity Language Model&lt;/a&gt; which augments a classical LSTM by explicitly modeling an arbitrary number of entities, updating their representations along the hidden state, and using the mention representations to contextually generate the next word in the LM task.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;To read more about that, check &lt;a href=&#34;https://sites.google.com/site/repl4nlp2018/&#34; target=&#34;_blank&#34;&gt;Yejin Choi‚Äôs talk at ACL 2018&lt;/a&gt; and &lt;a href=&#34;https://www.youtube.com/watch?v=7CcSm0PAr-Y&#34; target=&#34;_blank&#34;&gt;Percy Liang‚Äôs talk at AAAI 2018&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;the-big-open-question&#34;&gt;The Big Open Question&lt;/h2&gt;

&lt;p&gt;In the end, I feel like the main original question stayed open: can a model learn some aspects of lexical meaning from raw text alone?&lt;/p&gt;

&lt;h2 id=&#34;a-word-on-searle-s-chinese-room&#34;&gt;A word on Searle‚Äôs Chinese Room&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Chinese_room&#34; target=&#34;_blank&#34;&gt;Searle‚Äôs room argument&lt;/a&gt; came back often in the discussion but the situation was a bit different.&lt;/p&gt;

&lt;p&gt;Searle‚Äôs argument was made in the Strong versus Weak AI debate: does the computer has a mind or consciousness. Here the question is less philosophical: can we extract a representation of meaning from form alone.&lt;/p&gt;

&lt;p&gt;Still, &lt;a href=&#34;https://twitter.com/jeremyphoward/status/1026881686359822337&#34; target=&#34;_blank&#34;&gt;as Jeremy Howard detailed a bit later&lt;/a&gt;, the Chinese room experiment of Searle goes far beyond the question of strong/weak AI to the question of understanding/qualia, so please go &lt;a href=&#34;https://twitter.com/jeremyphoward/status/1026881686359822337&#34; target=&#34;_blank&#34;&gt;check this thread&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Filtered-Space Saving Top-K</title>
      <link>/post/topk-in-stream/</link>
      <pubDate>Mon, 25 Jun 2018 00:00:00 +0800</pubDate>
      
      <guid>/post/topk-in-stream/</guid>
      <description>

&lt;p&gt;Before diving into &lt;strong&gt;Filtered-Space Saving algorithm&lt;/strong&gt;, let us talk about the &lt;strong&gt;Space-Saving algorithm&lt;/strong&gt; first, it was proposed by Metwally et al. in [1]. Space-Saving underlying idea is to monitor only a pre-defined number of &lt;code&gt;m&lt;/code&gt; elements and their associated counters. Counters on each element are updated to reflect the maximum possible number of times an element has been observed and the error that might be involved in that estimate. If an element that is already being monitored occurs again, the counter for the frequency estimate is incremented. If the element is not currently monitored it is always added to the list. If the maximum number of elements has been reached, the element with the lower estimate of possible occurences is dropped. The new element estimate error is set to the estimate of frequency of the dropped element. The new element frequency estimate equal to the error plus &lt;code&gt;1&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The Space-Saving algorithm will keep in the list all the elements that may have occurred at least the new estimate error value (or the last dropped element estimate) of times. This ensures that no false negatives are generated but allows for false positives. Elements with low frequencies that are observed in the end of the data stream have higher probabilities of being present in the list.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.l2f.inesc-id.pt/~fmmb/wiki/uploads/Work/dict.refd.pdf&#34; target=&#34;_blank&#34;&gt;Filtered-Space Saving&lt;/a&gt; (FSS), that uses a filtering approach to improve on Space-Saving is a data structure and algorithm combination useful for accurately estimating the top k most frequent values appearing in a stream while using a constant, minimal memory footprint. The obvious approach to computing top-k is to simply keep a table of values and their associated frequencies, which is not practical for streams.&lt;/p&gt;

&lt;p&gt;Instead, FSS works by hashing incoming values into buckets, where each bucket has a collection of values already added. If the incoming element already exists at a given bucket, its frequency is incremented. If the element doesn‚Äôt exist, it will be added as long as a few certain configurable conditions are met.&lt;/p&gt;

&lt;p&gt;A golang version implementation of FSS is &lt;a href=&#34;https://github.com/liuzl/topk&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;A. Metwally, D. Agrawal, A. Abbadi, &lt;a href=&#34;http://www.cse.ust.hk/~raywong/comp5331/References/EfficientComputationOfFrequentAndTop-kElementsInDataStreams.pdf&#34; target=&#34;_blank&#34;&gt;Efficient Computation of Frequent and Top-k Elements in Data Streams&lt;/a&gt;, &lt;em&gt;Technical Report 2005-23, University of
California, Santa Barbara&lt;/em&gt;, 2005.&lt;/li&gt;
&lt;li&gt;Homem, Nuno, and J. P. Carvalho. &lt;a href=&#34;http://www.l2f.inesc-id.pt/~fmmb/wiki/uploads/Work/dict.refd.pdf&#34; target=&#34;_blank&#34;&gt;Finding top-k elements in data streams&lt;/a&gt;, &lt;em&gt;Elsevier Science Inc.&lt;/em&gt;, 2010.&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>pullword: Unsupervised Word Discovery</title>
      <link>/project/pullword/</link>
      <pubDate>Sat, 16 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/project/pullword/</guid>
      <description>

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;With the growing availability of digitized text data, there is a great need for effective computational tools to automatically extract kownledge from texts.&lt;/p&gt;

&lt;p&gt;The Chinese language differs most significantly from alphabet-based languages in not specifying word boundaries, most existing Chinese text-mining methods require a prespecified vocabulary and/or a large relevant training corpus, which may not be available in some applications.&lt;/p&gt;

&lt;p&gt;Pullword is developed for word discovering from small/large volumes of unstructured Chinese texts, and propose ways to order discovered words and conduct higher-level context analyses and it is particularly useful for mining online and domain-specific texts where the underlying vocabulary is unknown or the texts of interest differ significantly from available training corpora.&lt;/p&gt;

&lt;h2 id=&#34;implementations&#34;&gt;Implementations&lt;/h2&gt;

&lt;p&gt;The implementations mainly follow this &lt;a href=&#34;http://www.matrix67.com/blog/archives/5044&#34; target=&#34;_blank&#34;&gt;post&lt;/a&gt;.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/liuzl/pullword&#34; target=&#34;_blank&#34;&gt;golang version&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/nlpclub/nlpclub.github.io/blob/master/pullword/js/pullword.js&#34; target=&#34;_blank&#34;&gt;javascript version&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;online-demo&#34;&gt;Online demo&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://nlpclub.github.io/pullword/&#34; target=&#34;_blank&#34;&gt;javascript demo&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>q</title>
      <link>/q/</link>
      <pubDate>Thu, 07 Jun 2018 10:00:00 +0000</pubDate>
      
      <guid>/q/</guid>
      <description></description>
    </item>
    
    <item>
      <title>filestore</title>
      <link>/filestore/</link>
      <pubDate>Wed, 23 May 2018 10:00:00 +0000</pubDate>
      
      <guid>/filestore/</guid>
      <description></description>
    </item>
    
    <item>
      <title>G√∂del‚Äôs First Incompleteness Theorem for Programmers</title>
      <link>/post/godels-first-incompleteness-theorem/</link>
      <pubDate>Sun, 08 Apr 2018 00:00:00 +0800</pubDate>
      
      <guid>/post/godels-first-incompleteness-theorem/</guid>
      <description>

&lt;p&gt;G√∂del‚Äôs incompleteness theorems have been hailed as ‚Äúthe greatest mathematical discoveries of the 20th century‚Äù ‚Äî indeed, the theorems apply not only to mathematics, but all formal systems and have deep implications for science, logic, computer science, philosophy, and so on. In this post, I‚Äôll give a simple but rigorous sketch of G√∂del‚Äôs First Incompleteness Theorem. Formally, it states that:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Any consistent formal system $S$ within which a ‚Äúcertain amount of elementary arithmetic‚Äù can be carried out is incomplete; i.e., there are statements of the language of $S$ which can neither be proved nor disproved in $S$.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This is a rigorous proof with a focus on software engineers and programmers. I based this post on this &lt;a href=&#34;https://www.youtube.com/watch?v=9JeIG_CsgvI&#34; target=&#34;_blank&#34;&gt;excellent lecture&lt;/a&gt; I found on YouTube a little while ago. When I first learned how to prove G√∂del‚Äôs incompleteness theorems, it was in the context of a metalogic class that dealt with all kinds of confusing and deep topics like Henkin construction and transfinitary logic. But as it turns out, G√∂del can be understood without much fanfare!&lt;/p&gt;

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;We start on this journey by defining a couple of things. First, we define $F$ as a function that takes a positive integer and returns either &lt;code&gt;0&lt;/code&gt; or &lt;code&gt;1&lt;/code&gt;. Here‚Äôs an example of such an $F$:&lt;/p&gt;

&lt;p&gt;$$
isOdd(x)=\begin{cases}
0 &amp;amp; x\text{ is even}\\
1 &amp;amp; x\text{ is odd}
\end{cases}
$$&lt;/p&gt;

&lt;p&gt;So, we see that $isOdd(2)=0$ or that $isOdd(38943981)=1$. We can define $F$ however we want ‚Äî as long as the output will either be a &lt;code&gt;0&lt;/code&gt; or a &lt;code&gt;1&lt;/code&gt;. Let $Q$ be the set of all such functions $F$.&lt;/p&gt;

&lt;p&gt;We say that $F$ is computable if there exists a computer program (or proof) $P$ that takes as input $x$ and returns $F(x)$. It goes without saying that $P$ must complete within finite time and must be correct. So let‚Äôs look at some code. Is $isOdd(x)$ computable?&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-js&#34;&gt;function isOdd(x) {
  return (x % 2);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Looks like it! This program will always return &lt;code&gt;0&lt;/code&gt; when $x$ is even and &lt;code&gt;1&lt;/code&gt; when $x$ is odd. What about a more complicated example:&lt;/p&gt;

&lt;p&gt;$$
isPrime(x)=\begin{cases}
0 &amp;amp; x\text{ is not prime}\\
1 &amp;amp; x\text{ is prime}
\end{cases}
$$&lt;/p&gt;

&lt;p&gt;Is $isPrime$ computable?&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-js&#34;&gt;function isPrime(x) {
  if(x &amp;lt; 2) return 0;
  if(x == 2) return 1;
  for(var i = 2; i &amp;lt; x; i++) {
    if(x % i === 0) return 0;
  }
  return 1;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Courtesy of this &lt;a href=&#34;https://stackoverflow.com/a/48356406/243613&#34; target=&#34;_blank&#34;&gt;Stack Overflow answer&lt;/a&gt;, it looks like it is. Let $A$ be the set of all computable functions in $Q$. We just found out (the hard way) that $isOdd$ and $isPrime$ are in $A$ ‚Äî that is, they are computable.&lt;/p&gt;

&lt;p&gt;But here‚Äôs the big question: are all functions in $Q$ computable? Or, equivalently:&lt;/p&gt;

&lt;p&gt;$$
A\stackrel{?}{=}Q
$$&lt;/p&gt;

&lt;p&gt;If $A=Q$, then G√∂del was wrong, so we need to figure out a clever way to show that $A\subset Q$. In other words, we need to show that there are some functions that take positive integers $x$ as input and return either a &lt;code&gt;0&lt;/code&gt; or a &lt;code&gt;1&lt;/code&gt; that we simply &lt;strong&gt;cannot implement&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&#34;the-set-up&#34;&gt;The Set-up&lt;/h2&gt;

&lt;p&gt;So how do you show that some functions $F$ are not computable? Well, let‚Äôs do it the old-fashioned way. Since we‚Äôre already using JavaScript, let‚Äôs just print out every single JavaScript program. Ever. To make things easy, we can order them alphabetically and by length (in &lt;a href=&#34;https://en.wikipedia.org/wiki/Lexicographical_order&#34; target=&#34;_blank&#34;&gt;lexicographical order&lt;/a&gt;). To make things even easier, we can just throw out programs that loop infinitely or don‚Äôt return a &lt;code&gt;1&lt;/code&gt; or a &lt;code&gt;0&lt;/code&gt;. When all is said and done, we‚Äôre left with an infinite number of programs that probably start out like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-js&#34;&gt;function F1(x) {
  return 0;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-js&#34;&gt;function F2(x) {
  return 1;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And further down the line‚Ä¶&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-js&#34;&gt;function Fn(x) {
  return 1 - 1;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And further down the line‚Ä¶&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-js&#34;&gt;function Fn(x) {
  return x / x;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And further down the line‚Ä¶&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-js&#34;&gt;function Fn(x) {
  return (x % 2); // hey, this is the isOdd function from before!
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And even further down the line‚Ä¶&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-js&#34;&gt;function Fn(x) {
  return (x % 2) / 1;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And even further down the line‚Ä¶&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-js&#34;&gt;function Fn(x) {
  let someRandomVariable = x ^ x;
  let abcd = someRandomVariable / y;
  if (abdc &amp;gt; -12) return 0;
  return 1;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You get the picture. Now we have every single possible program written in JavaScript that outputs &lt;code&gt;0&lt;/code&gt; or &lt;code&gt;1&lt;/code&gt;. In other words, we just populated $A$: we now have a program that goes with every single computable function. Let‚Äôs put them in a big table called $T$. The column headings indicate inputs (positive integers) and the rows indicate computable functions (and, implicitly, the programs that implement them).&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;1&lt;/th&gt;
&lt;th&gt;2&lt;/th&gt;
&lt;th&gt;3&lt;/th&gt;
&lt;th&gt;4&lt;/th&gt;
&lt;th&gt;5&lt;/th&gt;
&lt;th&gt;6&lt;/th&gt;
&lt;th&gt;7&lt;/th&gt;
&lt;th&gt;&amp;hellip;&lt;/th&gt;
&lt;th&gt;n&lt;/th&gt;
&lt;th&gt;&amp;hellip;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;$f_1$&lt;/td&gt;
&lt;td&gt;$f_1(1)$&lt;/td&gt;
&lt;td&gt;$f_1(2)$&lt;/td&gt;
&lt;td&gt;$f_1(3)$&lt;/td&gt;
&lt;td&gt;$f_1(4)$&lt;/td&gt;
&lt;td&gt;$f_1(5)$&lt;/td&gt;
&lt;td&gt;$f_1(6)$&lt;/td&gt;
&lt;td&gt;$f_1(7)$&lt;/td&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;td&gt;$f_1(n)$&lt;/td&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;$f_2$&lt;/td&gt;
&lt;td&gt;$f_2(1)$&lt;/td&gt;
&lt;td&gt;$f_2(2)$&lt;/td&gt;
&lt;td&gt;$f_2(3)$&lt;/td&gt;
&lt;td&gt;$f_2(4)$&lt;/td&gt;
&lt;td&gt;$f_2(5)$&lt;/td&gt;
&lt;td&gt;$f_2(6)$&lt;/td&gt;
&lt;td&gt;$f_2(7)$&lt;/td&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;td&gt;$f_2(n)$&lt;/td&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;$f_{o}$&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;td&gt;$f_{o}(n)$&lt;/td&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;$f_{p}$&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;td&gt;$f_{p}(n)$&lt;/td&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;$f_i$&lt;/td&gt;
&lt;td&gt;$f_i(1)$&lt;/td&gt;
&lt;td&gt;$f_i(2)$&lt;/td&gt;
&lt;td&gt;$f_i(3)$&lt;/td&gt;
&lt;td&gt;$f_i(4)$&lt;/td&gt;
&lt;td&gt;$f_i(5)$&lt;/td&gt;
&lt;td&gt;$f_i(6)$&lt;/td&gt;
&lt;td&gt;$f_i(7)$&lt;/td&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;td&gt;$f_i(n)$&lt;/td&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&amp;hellip;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;You‚Äôll notice that $isOdd$ and $isPrime$ also made it in our table (as fo and fp, respectively). So far, so good. It seems that we thought of everything. But let‚Äôs define a new function:&lt;/p&gt;

&lt;p&gt;$$
\bar{f}(i)=1-f_i(i)
$$&lt;/p&gt;

&lt;p&gt;Where $f_i$ is the $i$th function in table $T$. First, let‚Äôs make sure we‚Äôre convinced that $\bar{f}$ is well-formed. The input $i$ is an integer. $f_{i}(i)$ will return either a &lt;code&gt;0&lt;/code&gt; or a &lt;code&gt;1&lt;/code&gt;, given that $f_i$ has a row populated in $T$. And finally, $1‚Äì0= 1$ or $1‚Äì1=0$, so both cases are well-formed.&lt;/p&gt;

&lt;p&gt;$$
\bar{f}(i)=\stackrel{\text{will return }0\text{ or }1}{\overbrace{1-\underset{\text{will return 0 or 1}}{\underbrace{f_{i}(i)}}}}
$$&lt;/p&gt;

&lt;p&gt;Therefore, $\bar{f}$ is in $Q$, but is it in $T$?&lt;/p&gt;

&lt;h2 id=&#34;the-proof&#34;&gt;The Proof&lt;/h2&gt;

&lt;p&gt;Seeing why $\bar{f}$ can‚Äôt be in $T$ is pretty straightforward. Suppose $\bar{f}$ is $f_2$. Like we‚Äôve seen so far, $f_2(2)$ can either be &lt;code&gt;0&lt;/code&gt; or &lt;code&gt;1&lt;/code&gt;. But if $f_2(2)=0$, then $\bar{f}(2)=1‚àíf_2(2)=1‚àí0=1$. So, we have:&lt;/p&gt;

&lt;p&gt;$$
f_2(2) \neq \bar{f}(2)
$$&lt;/p&gt;

&lt;p&gt;Whoops. Okay, so it wasn‚Äôt $f_2$. That would be too easy. What about $f_{421}$? If $f_{421}(421)=1$, then $\bar{f}(421)=1‚àíf_{421}(421)=1‚àí1=0$. So, we have:&lt;/p&gt;

&lt;p&gt;$$
f_{421}(421) \neq \bar{f}(421)
$$&lt;/p&gt;

&lt;p&gt;As it turns out, any computable function we pick out of $T$ (and implicitly $A$) will disagree with $\bar{f}$ at at least one output. And therefore, we have the amazing finding that:&lt;/p&gt;

&lt;p&gt;$$
A\subset Q
$$&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In other words, there are some things, like $\bar{f}$, we can‚Äôt prove or disprove in formal systems. Given that we‚Äôve been working with JavaScript in this post, it makes sense that it‚Äôs impossible to reference the program‚Äôs own lexicographic index in $T$. With that said, you might be tempted to think that there might be a way to ‚Äúget around‚Äù this limitation; if your language was clever enough, perhaps. I‚Äôll eventually write about G√∂del‚Äôs Second Incompleteness Theorem, which drives the nail in the coffin: there‚Äôs no way to get around this.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Weighted Random: algorithms for sampling from discrete probability distributions</title>
      <link>/post/weighted-random/</link>
      <pubDate>Thu, 05 Apr 2018 00:00:00 +0800</pubDate>
      
      <guid>/post/weighted-random/</guid>
      <description>

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;First of all what is weighted random? Let&amp;rsquo;s say you have a list of items and you want to pick one of them randomly. Doing this seems easy as all that&amp;rsquo;s required is to write a litte function that generates a random index referring to the one of the items in the list. But sometimes plain randomness is not enough, we want random results that are biased or based on some probability. This is where the weighted random generation algorithm needed.&lt;/p&gt;

&lt;h2 id=&#34;scenarios&#34;&gt;Scenarios&lt;/h2&gt;

&lt;p&gt;There are lots of real world scenarios that need weighted random. Such as load balancers(like &lt;a href=&#34;https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/&#34; target=&#34;_blank&#34;&gt;nginx&lt;/a&gt;, haproxy etc). Following is an example configuration of nginx. In the example, &lt;code&gt;backend1&lt;/code&gt; has weight &lt;code&gt;5&lt;/code&gt;; the other two servers have the default weight &lt;code&gt;1&lt;/code&gt;, With this configuration of weights, out of every seven requests, five are sent to &lt;code&gt;backend1&lt;/code&gt; and one to &lt;code&gt;backend2&lt;/code&gt; one to &lt;code&gt;backend3&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-nginx&#34;&gt;http {
    upstream backend {
        server backend1 weight=5;
        server backend2;
        server backend3;
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Another example is crawler scheduling. When I was developing a concurrent &lt;a href=&#34;https://github.com/crawlerclub/x&#34; target=&#34;_blank&#34;&gt;crawling framework&lt;/a&gt; last year, I need to schedule the crawling tasks according to the task importenceness. The tasks importenceness are expressed by float value weights that are mannually assigned to each site that tasks belong to. So there should be a &lt;a href=&#34;https://github.com/crawlerclub/x/blob/master/controller/crawler_scheduler.go#L45&#34; target=&#34;_blank&#34;&gt;WeightedChoice&lt;/a&gt; function on the scheduler of the crawler system that determines which task should be scheduled the next time.&lt;/p&gt;

&lt;p&gt;In the negative sampling part of the famous &lt;code&gt;word2vec&lt;/code&gt;, the algorithm needs to randomly sample some negative words according to their frequencies in the corpus. &lt;a href=&#34;https://github.com/tmikolov/word2vec/blob/master/word2vec.c#L527&#34; target=&#34;_blank&#34;&gt;Codes link&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;There are more examples in game developing: In games we often encounter random dropping of specified items by certain drop probability, such as falling silver coins 25%, gold coins 20%, diamonds 10%, equipment 5%, accessories 40%. The next dropped item type is now required to meet the above probability.&lt;/p&gt;

&lt;h2 id=&#34;solutions&#34;&gt;Solutions&lt;/h2&gt;

&lt;h3 id=&#34;solution-1&#34;&gt;Solution 1&lt;/h3&gt;

&lt;p&gt;The first method came up to me is to extend the uniform distributed random number generator. Let&amp;rsquo;s begin with an example(all example programmes here after will be writen in Golang):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;var items   = []int{0, 1, 2, 3}
var weights = []float32{0.1, 0.3, 0.4, 0.2}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Making them conform to the weights what we‚Äôd do is something simple. Basically repeat the items 10x or even 100x times based on the numbers we have. So let‚Äôs say we‚Äôre repeating 10x times, this is the list we‚Äôll end up with:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;var choices = []int{0, 1, 1, 1, 2, 2, 2, 2, 3, 3}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then we can randomly choose a value from &lt;code&gt;choices&lt;/code&gt;, and we are done. Full codes bellow:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;package main

import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;math/rand&amp;quot;
)

func WeightedRandomS1(weights []float32) int {
    if len(weights) == 0 {
        return 0
    }
    var choices []int
    for i, w := range weights {
        wi := int(w * 10)
        for j := 0; j &amp;lt; wi; j++ {
            choices = append(choices, i)
        }
    }
    return choices[rand.Int()%len(choices)]
}

func main() {
    for i := 0; i &amp;lt; 100; i++ {
        fmt.Println(WeightedRandom([]float32{0.1, 0.3, 0.6}))
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In &lt;code&gt;word2vec&lt;/code&gt;, this solution is adopted.&lt;/p&gt;

&lt;h3 id=&#34;solution-2&#34;&gt;Solution 2&lt;/h3&gt;

&lt;p&gt;The first solution takes too much memory, then came solution 2: Compute the discrete cumulative density function (CDF) of the list &amp;ndash; or in simple terms the array of cumulative sums of the weights. Then generate a random number in the range between 0 and the sum of all weights, do a linear search to find this random number in your discrete CDF array and get the value corresponding to this entry &amp;ndash; this is the weighted random number.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func WeightedRandomS2(weights []float32) int {
    if len(weights) == 0 {
        return 0
    }
    var sum float32 = 0.0
    for _, w := range weights {
        sum += w
    }
    r := rand.Float32() * sum
    for i, w := range weights {
        r -= w
        if r &amp;lt; 0 {
            return i
        }
    }
    return len(weights) - 1
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I used this solution in the scheduler of &lt;a href=&#34;https://github.com/crawlerclub/x&#34; target=&#34;_blank&#34;&gt;crawling framework&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;solution-3&#34;&gt;Solution 3&lt;/h3&gt;

&lt;p&gt;Adopting binary search over the CDF array could reduce the time complexity from $O(n)$ to $O(log(n))$.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;func WeightedRandomS3(weights []float32) int {
    n := len(weights)
    if n == 0 {
        return 0
    }
    cdf := make([]float32, n)
    var sum float32 = 0.0
    for i, w := range weights {
        if i &amp;gt; 0 {
            cdf[i] = cdf[i-1] + w
        } else {
            cdf[i] = w
        }
        sum += w
    }
    r := rand.Float32() * sum
    var l, h int = 0, n - 1
    for l &amp;lt;= h {
        m := l + (h-l)/2
        if r &amp;lt;= cdf[m] {
            if m == 0 || (m &amp;gt; 0 &amp;amp;&amp;amp; r &amp;gt; cdf[m-1]) {
                return m
            }
            h = m - 1
        } else {
            l = m + 1
        }
    }
    return -1
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;solution-4&#34;&gt;Solution 4&lt;/h3&gt;

&lt;p&gt;The optimal solution for weighted random should be the &lt;a href=&#34;https://en.wikipedia.org/wiki/Alias_method&#34; target=&#34;_blank&#34;&gt;Alias Method&lt;/a&gt;. It requires $O(n)$ time to initialize, $O(1)$ time to make a selection, and $O(n)$ memory. A golang version implementation is &lt;a href=&#34;https://github.com/liuzl/alias&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&#34;algorithm-vose-s-alias-method&#34;&gt;Algorithm: Vose&amp;rsquo;s Alias Method&lt;/h4&gt;

&lt;h5 id=&#34;initialization&#34;&gt;Initialization:&lt;/h5&gt;

&lt;ol&gt;
&lt;li&gt;Create arrays $Alias$ and $Prob$, each of size $n$.&lt;/li&gt;
&lt;li&gt;Create two worklists, $Small$ and $Large$.&lt;/li&gt;
&lt;li&gt;Multiply each probability by $n$.&lt;/li&gt;
&lt;li&gt;For each scaled probability $p_i$:

&lt;ol&gt;
&lt;li&gt;If $p_i&amp;lt;1$, add $i$ to $Small$.&lt;/li&gt;
&lt;li&gt;Otherwise $p_i \geqslant 1$, add $i$ to $Large$.&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;While $Small$ and $Large$ are not empty: ($Large$ might be emptied first)

&lt;ol&gt;
&lt;li&gt;Remove the first element from $Small$; call it $l$.&lt;/li&gt;
&lt;li&gt;Remove the first element from $Large$; call it $g$.&lt;/li&gt;
&lt;li&gt;Set $Prob[l]=p_l$.&lt;/li&gt;
&lt;li&gt;Set $Alias[l]=g$.&lt;/li&gt;
&lt;li&gt;Set $p_g = p_g + p_l - 1$. (This is a more numerically stable option)&lt;/li&gt;
&lt;li&gt;If $p_g&amp;lt;1$, add $g$ to $Small$.&lt;/li&gt;
&lt;li&gt;Otherwise $p_g \geqslant 1$, add $g$ to $Large$.&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;While $Large$ is not empty:

&lt;ol&gt;
&lt;li&gt;Remove the first element from $Large$; call it $g$.&lt;/li&gt;
&lt;li&gt;Set $Prob[g] = 1$.&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;While $Small$ is not empty: This is only possible due to numerical instability.

&lt;ol&gt;
&lt;li&gt;Remove the first element from $Small$; call it $l$.&lt;/li&gt;
&lt;li&gt;Set $Prob[l] = 1$.&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h5 id=&#34;generation&#34;&gt;Generation:&lt;/h5&gt;

&lt;ol&gt;
&lt;li&gt;Generate a fair die roll from an n-sided die; call the side $i$.&lt;/li&gt;
&lt;li&gt;Flip a biased coin that comes up heads with probability $Prob[i]$.&lt;/li&gt;
&lt;li&gt;If the coin comes up &amp;ldquo;heads&amp;rdquo;, return $i$.&lt;/li&gt;
&lt;li&gt;Otherwise, return $Alias[i]$.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Walker, A. J. 1977. &amp;ldquo;An efficient method for generating discrete random variable with general distributions.&amp;rdquo; &lt;em&gt;ACM Transactions on Mathematical Software&lt;/em&gt; &lt;strong&gt;3&lt;/strong&gt; 253‚Äì256.&lt;/li&gt;
&lt;li&gt;&amp;ldquo;&lt;a href=&#34;http://www.keithschwarz.com/darts-dice-coins/&#34; target=&#34;_blank&#34;&gt;Darts, Dice, and Coins: Sampling from a Discrete Distribution&lt;/a&gt;&amp;rdquo;. &lt;em&gt;Keith Schwarz&lt;/em&gt;, December 29, 2011&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Alias_method&#34; target=&#34;_blank&#34;&gt;Alias method&lt;/a&gt;. &lt;em&gt;Wikipedia&lt;/em&gt;, April 5, 2018&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://stackoverflow.com/questions/4463561/weighted-random-selection-from-array&#34; target=&#34;_blank&#34;&gt;Weighted random selection from array
&lt;/a&gt;, &lt;em&gt;stackoverflow&lt;/em&gt;, Dec 16 2010&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>FMR: functional meaning representation</title>
      <link>/project/fmr/</link>
      <pubDate>Fri, 16 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/project/fmr/</guid>
      <description>

&lt;h2 id=&#34;element-types-in-fmr&#34;&gt;Element types in FMR&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Constants&lt;/strong&gt;: Refer to specific objects in the world. A constant can be a number, a lexical string, or an entity.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Classes&lt;/strong&gt;: Semantic category of entities. For example: &lt;code&gt;location.city&lt;/code&gt;, &lt;code&gt;math.number&lt;/code&gt;

&lt;ul&gt;
&lt;li&gt;Sub-class: &lt;em&gt;math.number ‚äÜ math.expression&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Template classes: classes with one or more parameters, for example: &lt;code&gt;t.list&amp;lt;c, m, n&amp;gt;&lt;/code&gt;, where &lt;code&gt;c&lt;/code&gt; is a class, &lt;code&gt;m&lt;/code&gt; and &lt;code&gt;n&lt;/code&gt; are integers.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Functions&lt;/strong&gt;: The major way to form larger language units from smaller ones. A function is comprised of a &lt;em&gt;name&lt;/em&gt;, a list of &lt;em&gt;core arguments&lt;/em&gt;, and a &lt;em&gt;return type&lt;/em&gt;.

&lt;ul&gt;
&lt;li&gt;Noun functions

&lt;ul&gt;
&lt;li&gt;Map entities to their properties or to other entities having specific relations withe the argument(s).&lt;/li&gt;
&lt;li&gt;Are used to represent noun phrases in natural language.&lt;/li&gt;
&lt;li&gt;Pronoun functions are special zero-argument noun functions.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Verb functions

&lt;ul&gt;
&lt;li&gt;Act as sentences or sub-sentences&lt;/li&gt;
&lt;li&gt;The most important function type?&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Modifier functions

&lt;ul&gt;
&lt;li&gt;Many functions can take additional &lt;em&gt;extended arguments&lt;/em&gt; as their modifiers.&lt;/li&gt;
&lt;li&gt;Modifier functions often take the role of extended arguments to modify noun function, verb functions or other modifier functions.&lt;/li&gt;
&lt;li&gt;Are used in FMR as the semantic representation of adjectives, adverb phrases (including conjunctive adverb phrases), and prepositional phrases in NL&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Entity Variables&lt;/strong&gt;: Variables are assigned to FMR nodes for indicating the co-reference of sub-trees to entities.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;features-of-fmr&#34;&gt;Features of FMR&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Strongly typed language

&lt;ul&gt;
&lt;li&gt;Type-compatibility: The type of each child of a function node should match the corresponding argument type of the function.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Open-domain type system.

&lt;ul&gt;
&lt;li&gt;Thousands of types&lt;/li&gt;
&lt;li&gt;Other languages: At most 100+ in grammar level&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Built-in data structures

&lt;ul&gt;
&lt;li&gt;like &lt;code&gt;t.list&lt;/code&gt; and &lt;code&gt;nf.list&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;semantic-parsing-from-nl-to-fmr&#34;&gt;Semantic parsing from NL to FMR&lt;/h2&gt;

&lt;p&gt;By using CFG rules to map natural language sentences and phrases to FMR&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;&amp;quot;The product of 3 consecutive number is 60&amp;quot; =&amp;gt; 
vf.be.equ(nf.math.product(nf.list(math.number, 3, mf.consecutive)), 60);
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;ÁºòËµ∑&#34;&gt;ÁºòËµ∑&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;ËØ≠‰πâÁêÜËß£Âπ≥Âè∞/ËØ≠Èü≥Âä©ÊâãÔºà2013-‰ªäÔºåËÖæËÆØ/ÊêúÁãóÔºâ

&lt;ul&gt;
&lt;li&gt;Â§çÂêàÂÆû‰ΩìËØÜÂà´ÔºàCRFËØÜÂà´/ËßÑÂàôËØÜÂà´/ËØçË°®ËØÜÂà´Ôºâ&lt;/li&gt;
&lt;li&gt;Âü∫‰∫éËØ≠Ë®ÄÊ®°ÂûãÁöÑÊü•ËØ¢ÊÑèÂõæÁêÜËß£&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Âü∫‰∫éËØ≠‰πâÊ®°ÊùøÁöÑÊü•ËØ¢ÊÑèÂõæËØÜÂà´ÂíåËØ≠‰πâ‰ø°ÊÅØÊäΩÂèñ&lt;/strong&gt;

&lt;ul&gt;
&lt;li&gt;ÈÄöÁî®CFGÊñáÊ≥ïËß£ÊûêÂô®ÔºöÈ´òÊïàEarleyÁÆóÊ≥ïÁöÑÂÆûÁé∞&lt;/li&gt;
&lt;li&gt;ÊÑèÂõæËØÜÂà´ÔºåÊßΩ‰ΩçÂ°´ÂÖÖ&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Êü•ËØ¢ÊÑèÂõæÊéíÂ∫èÔºåÂü∫‰∫éÂÄôÈÄâÊÑèÂõæÁªìÊûúÁöÑÁªàÂà§Ê®°Âùó&lt;/li&gt;
&lt;li&gt;‰∏ãÊ∏∏ÂàÜÁ±ªÂà´Áü•ËØÜÂ∫ìÁ≥ªÁªü/ÂûÇÁõ¥ÊêúÁ¥¢Á≥ªÁªü&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>gocc: Golang version OpenCC ÁπÅÁ∞°ËΩâÊèõ</title>
      <link>/project/gocc/</link>
      <pubDate>Fri, 16 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/project/gocc/</guid>
      <description>

&lt;h2 id=&#34;introduction-‰ªãÁ¥π&#34;&gt;Introduction ‰ªãÁ¥π&lt;/h2&gt;

&lt;p&gt;gocc is a golang port of OpenCC(&lt;a href=&#34;https://github.com/BYVoid/OpenCC/&#34; target=&#34;_blank&#34;&gt;Open Chinese Convert ÈñãÊîæ‰∏≠ÊñáËΩâÊèõ&lt;/a&gt;) which is a project for conversion between Traditional and Simplified Chinese developed by &lt;a href=&#34;https://www.byvoid.com/&#34; target=&#34;_blank&#34;&gt;BYVoid&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;gocc stands for &amp;ldquo;&lt;strong&gt;Go&lt;/strong&gt;lang version Open&lt;strong&gt;CC&lt;/strong&gt;&amp;rdquo;, it is a total rewrite version of OpenCC by Go. It just borrows the dict files and config files of OpenCC, so it may not produce the same output with the original OpenCC.&lt;/p&gt;

&lt;h2 id=&#34;installation-ÂÆâË£ù&#34;&gt;Installation ÂÆâË£ù&lt;/h2&gt;

&lt;h3 id=&#34;1-golang-package&#34;&gt;1, golang package&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;go get github.com/liuzl/gocc
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;2-command-line&#34;&gt;2, Command Line&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git clone https://github.com/liuzl/gocc
cd gocc/cmd
make install
gocc --help
echo &amp;quot;Êàë‰ª¨ÊòØÂ∑•ÂÜúÂ≠êÂºüÂÖµ&amp;quot; | gocc
#ÊàëÂÄëÊòØÂ∑•Ëæ≤Â≠êÂºüÂÖµ
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;usage-‰ΩøÁî®&#34;&gt;Usage ‰ΩøÁî®&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;package main

import (
    &amp;quot;fmt&amp;quot;
    &amp;quot;github.com/liuzl/gocc&amp;quot;
    &amp;quot;log&amp;quot;
)

func main() {
    s2t, err := gocc.New(&amp;quot;s2t&amp;quot;)
    if err != nil {
        log.Fatal(err)
    }
    in := `Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÊòØ‰∫∫Â∑•Êô∫ËÉΩÈ¢ÜÂüü‰∏≠ÁöÑ‰∏Ä‰∏™ÈáçË¶ÅÊñπÂêë„ÄÇ`
    out, err := s2t.Convert(in)
    if err != nil {
        log.Fatal(err)
    }
    fmt.Printf(&amp;quot;%s\n%s\n&amp;quot;, in, out)
    //Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÊòØ‰∫∫Â∑•Êô∫ËÉΩÈ¢ÜÂüü‰∏≠ÁöÑ‰∏Ä‰∏™ÈáçË¶ÅÊñπÂêë„ÄÇ
    //Ëá™ÁÑ∂Ë™ûË®ÄËôïÁêÜÊòØ‰∫∫Â∑•Êô∫ËÉΩÈ†òÂüü‰∏≠ÁöÑ‰∏ÄÂÄãÈáçË¶ÅÊñπÂêë„ÄÇ
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;conversions&#34;&gt;Conversions&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;s2t&lt;/code&gt; Simplified Chinese to Traditional Chinese&lt;/li&gt;
&lt;li&gt;&lt;code&gt;t2s&lt;/code&gt; Traditional Chinese to Simplified Chinese&lt;/li&gt;
&lt;li&gt;&lt;code&gt;s2tw&lt;/code&gt; Simplified Chinese to Traditional Chinese (Taiwan Standard)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tw2s&lt;/code&gt; Traditional Chinese (Taiwan Standard) to Simplified Chinese&lt;/li&gt;
&lt;li&gt;&lt;code&gt;s2hk&lt;/code&gt; Simplified Chinese to Traditional Chinese (Hong Kong Standard)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;hk2s&lt;/code&gt; Traditional Chinese (Hong Kong Standard) to Simplified Chinese&lt;/li&gt;
&lt;li&gt;&lt;code&gt;s2twp&lt;/code&gt; Simplified Chinese to Traditional Chinese (Taiwan Standard) with Taiwanese idiom&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tw2sp&lt;/code&gt; Traditional Chinese (Taiwan Standard) to Simplified Chinese with Mainland Chinese idiom&lt;/li&gt;
&lt;li&gt;&lt;code&gt;t2tw&lt;/code&gt; Traditional Chinese (OpenCC Standard) to Taiwan Standard&lt;/li&gt;
&lt;li&gt;&lt;code&gt;t2hk&lt;/code&gt; Traditional Chinese (OpenCC Standard) to Hong Kong Standard&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>goutil</title>
      <link>/goutil/</link>
      <pubDate>Mon, 20 Nov 2017 10:00:00 +0000</pubDate>
      
      <guid>/goutil/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
