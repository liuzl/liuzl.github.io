<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="theme" content="hugo-academic">
  <meta name="generator" content="Hugo 0.53" />
  <meta name="author" content="Zhanliang Liu">

  
  
  
  
    
      
    
  
  <meta name="description" content="I found a very worthwhile article while surfing medium.com days ago. The article is a summary of a twitter thread which talked about meaning, semantics, language models, learning Thai and Java, entailment, co-reference‚Ää‚Äî‚Ääall in one fascinating thread. The original article is here.
Following is a copy of the original article:
Last week a tweet by Jacob Andreas triggered a huge discussion on Twitter that many people have called the meaning/semantics mega-thread.">

  
  <link rel="alternate" hreflang="en-us" href="/post/semantic-meaning/">

  


  

  
  
  <meta name="theme-color" content="#0095eb">
  
  
  
  
    
  
  
    
    
      
        <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
      
    
  
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha512-6MXa8B6uaO18Hid6blRMetEIoPqHf7Ux1tnyIQdpt9qI5OACx7C+O3IVTr98vwGnlcg0LOLa02i9Y1HpVhlfiw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.1/css/academicons.min.css" integrity="sha512-NThgw3XKQ1absAahW6to7Ey42uycrVvfNfyjqcFNgCmOCQ5AR4AO0SiXrN+8ZtYeappp56lk1WtvjVmEa+VR6A==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha512-SfTiTlX6kk+qitfevl/7LibUOeJWlt9rbyDn92a1DqWOw9vWG2MFoays0sgObmWazO5BQPiFucnnEAjpAB+/Sw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">
  
  
  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700%7cRoboto:400,400italic,700%7cRoboto&#43;Mono">
  
  <link rel="stylesheet" href="/styles.css">
  

  
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-120921582-1', 'auto');
      ga('require', 'eventTracker');
      ga('require', 'outboundLinkTracker');
      ga('require', 'urlChangeTracker');
      ga('send', 'pageview');
    </script>
    <script async src="//www.google-analytics.com/analytics.js"></script>
    
    <script async src="https://cdnjs.cloudflare.com/ajax/libs/autotrack/2.4.1/autotrack.js" integrity="sha512-HUmooslVKj4m6OBu0OgzjXXr+QuFYy/k7eLI5jdeEy/F4RSgMn6XRWRGkFi5IFaFgy7uFTkegp3Z0XnJf3Jq+g==" crossorigin="anonymous"></script>
    
  

  
  <script type="text/javascript" src="//tajs.qq.com/stats?sId=65896196" charset="UTF-8"></script>
  

  
  <link rel="alternate" href="/index.xml" type="application/rss+xml" title="Zhanliang Liu">
  <link rel="feed" href="/index.xml" type="application/rss+xml" title="Zhanliang Liu">
  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="/post/semantic-meaning/">

  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="twitter:site" content="@liuzl">
  <meta property="twitter:creator" content="@liuzl">
  
  <meta property="og:site_name" content="Zhanliang Liu">
  <meta property="og:url" content="/post/semantic-meaning/">
  <meta property="og:title" content="‚õµ Learning Meaning in Natural Language Processing‚Ää‚Äî‚ÄäThe Semantics Mega-Thread | Zhanliang Liu">
  <meta property="og:description" content="I found a very worthwhile article while surfing medium.com days ago. The article is a summary of a twitter thread which talked about meaning, semantics, language models, learning Thai and Java, entailment, co-reference‚Ää‚Äî‚Ääall in one fascinating thread. The original article is here.
Following is a copy of the original article:
Last week a tweet by Jacob Andreas triggered a huge discussion on Twitter that many people have called the meaning/semantics mega-thread.">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2018-11-15T00:00:00&#43;08:00">
  
  <meta property="article:modified_time" content="2018-11-15T00:00:00&#43;08:00">
  

  

  <title>‚õµ Learning Meaning in Natural Language Processing‚Ää‚Äî‚ÄäThe Semantics Mega-Thread | Zhanliang Liu</title>

</head>
<body id="top" data-spy="scroll" data-target="#toc" data-offset="71" >

<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
  <div class="container">

    
    <div class="navbar-header">
      
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target=".navbar-collapse" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
      <a class="navbar-brand" href="/">Zhanliang Liu</a>
    </div>

    
    <div class="collapse navbar-collapse">

      
      
      <ul class="nav navbar-nav navbar-right">
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#about">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#projects">
            
            <span>Projects</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#posts">
            
            <span>Posts</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#talks">
            
            <span>Talks</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#contact">
            
            <span>Contact</span>
            
          </a>
        </li>

        
        
      

      
      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">

  


  <div class="article-container">
    <h1 itemprop="name">‚õµ Learning Meaning in Natural Language Processing‚Ää‚Äî‚ÄäThe Semantics Mega-Thread</h1>

    

<div class="article-metadata">

  <span class="article-date">
    
    <time datetime="2018-11-15 00:00:00 &#43;0800 CST" itemprop="datePublished dateModified">
      Nov 15, 2018
    </time>
  </span>
  <span itemscope itemprop="author publisher" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Zhanliang Liu">
  </span>

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    8 min read
  </span>
  

  
  

  

  
  
<div class="share-box" aria-hidden="true">
  <ul class="share">
    <li>
      <a class="twitter"
         href="https://twitter.com/intent/tweet?text=%e2%9b%b5%20Learning%20Meaning%20in%20Natural%20Language%20Processing%e2%80%8a%e2%80%94%e2%80%8aThe%20Semantics%20Mega-Thread&amp;url=%2fpost%2fsemantic-meaning%2f"
         target="_blank" rel="noopener">
        <i class="fa fa-twitter"></i>
      </a>
    </li>
    <li>
      <a class="facebook"
         href="https://www.facebook.com/sharer.php?u=%2fpost%2fsemantic-meaning%2f"
         target="_blank" rel="noopener">
        <i class="fa fa-facebook"></i>
      </a>
    </li>
    <li>
      <a class="linkedin"
         href="https://www.linkedin.com/shareArticle?mini=true&amp;url=%2fpost%2fsemantic-meaning%2f&amp;title=%e2%9b%b5%20Learning%20Meaning%20in%20Natural%20Language%20Processing%e2%80%8a%e2%80%94%e2%80%8aThe%20Semantics%20Mega-Thread"
         target="_blank" rel="noopener">
        <i class="fa fa-linkedin"></i>
      </a>
    </li>
    <li>
      <a class="weibo"
         href="http://service.weibo.com/share/share.php?url=%2fpost%2fsemantic-meaning%2f&amp;title=%e2%9b%b5%20Learning%20Meaning%20in%20Natural%20Language%20Processing%e2%80%8a%e2%80%94%e2%80%8aThe%20Semantics%20Mega-Thread"
         target="_blank" rel="noopener">
        <i class="fa fa-weibo"></i>
      </a>
    </li>
    <li>
      <a class="email"
         href="mailto:?subject=%e2%9b%b5%20Learning%20Meaning%20in%20Natural%20Language%20Processing%e2%80%8a%e2%80%94%e2%80%8aThe%20Semantics%20Mega-Thread&amp;body=%2fpost%2fsemantic-meaning%2f">
        <i class="fa fa-envelope"></i>
      </a>
    </li>
  </ul>
</div>


  

</div>


    <div class="article-style" itemprop="articleBody">
      

<p>I found a very worthwhile article while surfing medium.com days ago. The article is a summary of a twitter thread which talked about meaning, semantics, language models, learning Thai and Java, entailment, co-reference‚Ää‚Äî‚Ääall in one fascinating thread. The original article is <a href="https://medium.com/huggingface/learning-meaning-in-natural-language-processing-the-semantics-mega-thread-9c0332dfe28e" target="_blank">here</a>.</p>

<p>Following is a copy of the original article:</p>

<p>Last week <a href="https://twitter.com/jacobandreas/status/1023246560082063366" target="_blank">a tweet by Jacob Andreas</a> triggered a huge discussion on Twitter that many people have called the <em>meaning/semantics mega-thread</em>.</p>

<h2 id="a-crash-course-on-lexical-meaning-semantics">üèé A crash course on Lexical Meaning &amp; Semantics</h2>

<p>If you already know what we mean by <em>‚ÄúMeaning/Semantics‚Äù</em> in NLP, you can skip this part and go straight to the debate üî•.</p>

<p>For the CS/ML folks out there here are a few words of introduction.</p>

<p>First, it‚Äôs important to state that Meaning in Natural Language is a multi-facetted concept with semantic, pragmatic, cognitive and social aspects. The discussion that happened on Twitter was mainly about lexical semantics and compositionality so I will focus on this sub-field for brevity. You will find additional links to broaden this view at the end of this section.</p>

<blockquote>
<p><a href="https://en.wikipedia.org/wiki/Meaning_%28linguistics%29" target="_blank">Meaning</a> is the information that a sender intends to convey, or does convey, to a receiver.</p>
</blockquote>

<p>Now, we know that <strong>strings are already a representation of meaning</strong>, so why should we go any further than just raw text?</p>

<p>Well there are several reasons we may want to distinguish meaning from raw text.</p>

<p>One reason is that the field of NLP/NLU aims at <strong>building systems</strong> that understand what you say to them, trigger actions based on that and convey back meaningful information. Let‚Äôs take a simple example:</p>

<pre><code>Context: Knowledge of mathematics
Utterance: What is the largest prime less than 10?
Action: 7
</code></pre>

<p>Given some knowledge of math, we want our NLU system to produce an appropriate answer.</p>

<p>It‚Äôs difficult to (i) link raw text to a knowledge base of mathematical facts in our system and (ii) combine pieces of knowledge together to infer an answer. One solution is to define an intermediate meaning representation (sometimes called a Logical Form) that is more easy to manipulate.</p>

<p>For example in our case:</p>

<p>$$
Meaning = max(primes \cap (-\infty, 10))
$$</p>

<p>We can then execute this expression with respect to a model of the world, like our database of knowledge, to get an answer. This way, we have also factored out the understanding of language (called <em>semantic parsing</em>) from the world knowledge (the problem of grounding meaning of in the real word).</p>

<p>Advantageously, our <em>representation of the meaning of a sentence</em> can thus:</p>

<ol>
<li>provide a way to <strong>link language</strong> to external <strong>knowledge base</strong>, <strong>observations</strong>, and <strong>actions</strong>;</li>
<li>support <strong>computational inference</strong>, so that concepts can be <strong>combined</strong> to derive additional knowledge as human do during a conversation.</li>
</ol>

<p>Two other nice requirements for this representation:</p>

<ol>
<li><strong>unambiguous</strong>: one meaning per statement (unlike natural language);</li>
<li><strong>expressive</strong> enough to cover the full range of things that people talk about.</li>
</ol>

<p>Natural language as raw text doesn‚Äôt fulfill most of these criteria!</p>

<p>A related line of research is <strong>Formal Semantics</strong> which seek to understand linguistic meaning by constructing models of the principles that speakers use to convey meaning.</p>

<p>The tools of formal semantics are similar to NLU/NLP tools but the aim is to understand how people construct meaning more than any specific application.</p>

<p>Now there is a lot more to meaning than just logic forms and grounding. A few examples: <strong>‚ÄúBut I didn‚Äôt mean it literally!!‚Äù</strong> (speaker meaning ‚â† literal meaning), <strong>‚ÄúBuffalo buffalo Buffalo buffalo buffalo buffalo Buffalo buffalo.‚Äù</strong> (yes, this is a real sentence with a meaning but you need to find the right sense for each buffalo!) and so on&hellip;</p>

<blockquote>
<p>A few pointers: Our simple example came from <a href="https://cs.stanford.edu/~pliang/papers/executable-cacm2016.pdf" target="_blank">this nice article by Percy Liang</a>. As a quick overview of the field, I would recommend chapters 12 and 13 of J. Eisenstein‚Äôs book <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf" target="_blank">‚ÄúNatural Language Processing‚Äù</a>. They will take you through the main ideas, tools up to recent research on Meaning in NLP. <a href="http://faculty.washington.edu/ebender/100things-sem_prag.html" target="_blank">Emily M. Bender‚Äôs ACL 2018 tutorial</a> is a nice way to see how Meaning can be a multi-headed monster üêç to say the least!</p>
</blockquote>

<p>Now back to our mega-thread!</p>

<h2 id="triggering-a-debate-on-meaning">üî• Triggering a debate on Meaning</h2>

<p>As often, the discussion was sparked by a mention of sentence embeddings.</p>

<p>This argument was the main trigger for the mega-discussion that followed. Further in the thread, Emily M. Bender reformulates her argument as:</p>

<blockquote>
<p>If all the learner gets is text the learner cannot learn meaning.</p>
</blockquote>

<h2 id="can-a-model-trained-only-on-raw-text-learn-meaning">Can a model trained only on raw text learn meaning?</h2>

<p>This question was explored along two axes:</p>

<h3 id="what-aspect-of-meaning-can-a-model-learn">What aspect of Meaning can a model learn?</h3>

<ul>
<li><p>Can it learn <em>meaning</em> or just learn <em>similarity</em> in meaning (i.e. learn that some expressions are similar without knowing what they mean. It is still very useful for transfer learning)?</p></li>

<li><p>Can it learn <em>grounded meaning</em> (learn the meaning of each expression as a state of the world state) or learn <em>lexical meaning</em> (e.g. learn how the meaning of sub-expressions compose together, as in our logical forms)?</p></li>
</ul>

<h3 id="how-can-the-model-learn">How can the model Learn?</h3>

<ul>
<li><p>If the model cannot learn meaning from raw text alone, what would be the <em>minimal amount of additional supervision</em> needed? Should we add supervision from Logical Forms, Textual Entailment&hellip;?</p></li>

<li><p>Could we encode some <em>inductive bias</em> in the model so that it can learn aspects of meaning from raw text?</p></li>
</ul>

<h2 id="the-thai-and-java-experiments">The Thai and Java Experiments</h2>

<p>Emily M. Bender proposed several interesting experiments which were discussed at length:</p>

<ul>
<li><a href="https://twitter.com/emilymbender/status/1024042044035985408" target="_blank">The Thai Room experiment</a>: <em>‚ÄúImagine [you] were given the sum total of all Thai literature in a huge library. (All in Thai, no translations.) Assuming you don‚Äôt already know Thai, you won‚Äôt learn it from that.‚Äù</em> <strong>A real life example of trying to learn from raw text only.</strong></li>
<li><a href="https://twitter.com/emilymbender/status/1025002835467890689" target="_blank">The Java Code experiment</a>: <em>‚ÄúGive your NN all well-formed java code that‚Äôs ever been written, but only the surface form of the code. Then ask it to evaluate (i.e. execute) part of it.‚Äù</em> <strong>Can we learn execution semantics from raw text only?</strong></li>
</ul>

<h2 id="investigating-programming-language-semantic">Investigating Programming Language Semantic</h2>

<p>The Java Code proposal triggered an interesting discussion on the difference between trying to learn meaning from Programming Language (PL) code and from Natural Language (NL) text.</p>

<p>It actually seems more difficult to learn from PL than NL.</p>

<p>Learning meaning from Java code is like having a text composed only of orders/commands and without any descriptions. But description are very important feedbacks for learning as they allow one to compare its internal world state with the real world state.</p>

<h2 id="language-models">Language Models</h2>

<p>The discussion circled around Language Models. A language model is a model which can <strong>predict the next word in a sentence given a context</strong> (past words, external knowledge). Recently, these models have given <a href="https://blog.openai.com/language-unsupervised/" target="_blank">interesting results</a> in commonsense reasoning. Language models were examined in two settings:</p>

<ul>
<li><p><strong>Independently of any training dataset</strong>: having a <em>human-level language model</em> involves that the model has a human-like notion of meaning and world state. As <a href="https://twitter.com/yoavgo/status/1025485692347056128" target="_blank">Yolav Goldberg mentioned</a> <em>‚Äúit‚Äôs just (one of the many) trivial examples where perfect LM entails solving not only all of language but also all of AI.‚Äù</em></p></li>

<li><p>More interesting is the case of a language model <strong>trained from raw text only</strong>: here the question is how much the model can learn in terms of semantics without being given access to explicit meaning information!</p></li>
</ul>

<h2 id="textual-entailment">Textual Entailment</h2>

<p>One way to give some information about meaning without departing too much from raw text is to train a model on Textual Entailment, the task of <strong>predicting whether a sentence imply another</strong>.</p>

<p>In a series tweets, Sam Bowman explained his view that entailment could be used to learn ‚Äúthe meat of compositional‚Äù semantics almost from raw text.</p>

<p>And also made the suggestion that a learner might be able to learn entailment in a simpler way than curent setups, using a setup that could be close to LM.</p>

<h2 id="an-inductive-bias-language-model">An Inductive Bias Language Model</h2>

<p>Another way to learn meaning from a dataset as close as possible to raw text is to put a strong inductive bias in the model as discussed by Matt Gardner.</p>

<p>One example is the <a href="https://arxiv.org/abs/1708.00781" target="_blank">Entity Language Model</a> which augments a classical LSTM by explicitly modeling an arbitrary number of entities, updating their representations along the hidden state, and using the mention representations to contextually generate the next word in the LM task.</p>

<blockquote>
<p>To read more about that, check <a href="https://sites.google.com/site/repl4nlp2018/" target="_blank">Yejin Choi‚Äôs talk at ACL 2018</a> and <a href="https://www.youtube.com/watch?v=7CcSm0PAr-Y" target="_blank">Percy Liang‚Äôs talk at AAAI 2018</a>.</p>
</blockquote>

<h2 id="the-big-open-question">The Big Open Question</h2>

<p>In the end, I feel like the main original question stayed open: can a model learn some aspects of lexical meaning from raw text alone?</p>

<h2 id="a-word-on-searle-s-chinese-room">A word on Searle‚Äôs Chinese Room</h2>

<p><a href="https://en.wikipedia.org/wiki/Chinese_room" target="_blank">Searle‚Äôs room argument</a> came back often in the discussion but the situation was a bit different.</p>

<p>Searle‚Äôs argument was made in the Strong versus Weak AI debate: does the computer has a mind or consciousness. Here the question is less philosophical: can we extract a representation of meaning from form alone.</p>

<p>Still, <a href="https://twitter.com/jeremyphoward/status/1026881686359822337" target="_blank">as Jeremy Howard detailed a bit later</a>, the Chinese room experiment of Searle goes far beyond the question of strong/weak AI to the question of understanding/qualia, so please go <a href="https://twitter.com/jeremyphoward/status/1026881686359822337" target="_blank">check this thread</a>.</p>

    </div>

    


<div class="article-tags">
  
  <a class="btn btn-primary btn-outline" href="/tags/nlp">NLP</a>
  
  <a class="btn btn-primary btn-outline" href="/tags/nlu">NLU</a>
  
  <a class="btn btn-primary btn-outline" href="/tags/semantic">Semantic</a>
  
  <a class="btn btn-primary btn-outline" href="/tags/meaning">Meaning</a>
  
</div>




    
    
    <div class="article-widget">
      <div class="hr-light"></div>
      <h3>Related</h3>
      <ul>
        
        <li><a href="/project/fmr/">FMR: functional meaning representation</a></li>
        
        <li><a href="/project/pullword/">pullword: Unsupervised Word Discovery</a></li>
        
        <li><a href="/project/ling/">ling: A Natural Language Processing toolkit in Golang</a></li>
        
        <li><a href="/project/gocc/">gocc: Golang version OpenCC ÁπÅÁ∞°ËΩâÊèõ</a></li>
        
      </ul>
    </div>
    

    

    


  </div>
</article>

<footer class="site-footer">
  <div class="container">
    <p class="powered-by">

      &copy; 2021 &middot; 

      Powered by the
      <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
      <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

      <span class="pull-right" aria-hidden="true">
        <a href="#" id="back_to_top">
          <span class="button_icon">
            <i class="fa fa-chevron-up fa-2x"></i>
          </span>
        </a>
      </span>

    </p>
  </div>
</footer>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <button type="button" class="close btn-large" data-dismiss="modal">&times;</button>
        <h4 class="modal-title">Cite</h4>
      </div>
      <div>
        <pre><code class="modal-body tex"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-primary btn-outline js-copy-cite" href="#" target="_blank">
          <i class="fa fa-copy"></i> Copy
        </a>
        <a class="btn btn-primary btn-outline js-download-cite" href="#" target="_blank">
          <i class="fa fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    

    

    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js" integrity="sha512-3P8rXCuGJdNZOnUx/03c1jOTnMn3rP63nBip5gOP2qmUh5YAdVAvFZ1E+QLZZbC1rtMrQb+mah3AfYW11RUrWA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha512-iztkobsvnjKfAtTNdHkGVjAYTrrtlC7mGp/54c40wowO7LhURYl3gVzzcEqGl/qKXQltJ2HwMrdLcNUdo+N/RQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>
    
    
    <script src="/js/hugo-academic.js"></script>
    

    
    
      
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
      

      

      

      <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
    </script>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML" integrity="sha512-tOav5w1OjvsSJzePRtt2uQPFwBoHt1VZcUq8l8nm5284LEKE9FSJBQryzMBzHxY5P0zRdNqEcpLIRVYFNgu1jw==" crossorigin="anonymous"></script>
    
    

  </body>
</html>

